{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSure, here's a quick and dirty joke:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings()\n",
    "model.invoke(\"joke tell me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Machine Learning For Absolute\\nBeginners\\n \\n \\n \\n \\nOliver Theobald', metadata={'source': 'ml.pdf', 'page': 1}),\n",
       " Document(page_content='Second Edition\\nCopyright © 2017 by Oliver Theobald\\nAll rights reserved. No part of this publication may be reproduced,\\ndistributed, or transmitted in any form or by any means, including\\nphotocopying, recording, or other electronic or mechanical\\nmethods, without the prior written permission of the publisher,\\nexcept in the case of brief quotations embodied in critical reviews\\nand certain other non-commercial uses permitted by copyright law.', metadata={'source': 'ml.pdf', 'page': 2}),\n",
       " Document(page_content='Contents\\n \\nINTRODUCTION\\nWHAT IS MACHINE LEARNING?\\nML CATEGORIES\\nTHE ML TOOLBOX\\nDATA SCRUBBING\\nSETTING UP YOUR DATA\\nREGRESSION ANALYSIS\\nCLUSTERING\\nBIAS & VARIANCE\\nARTIFICIAL NEURAL NETWORKS\\nDECISION TREES\\nENSEMBLE MODELING\\nBUILDING A MODEL IN PYTHON\\nMODEL OPTIMIZATION\\nFURTHER RESOURCES\\nDOWNLOADING DATASETS\\nFINAL WORD', metadata={'source': 'ml.pdf', 'page': 3}),\n",
       " Document(page_content='INTRODUCTION\\nMachines have come a long way since the Industrial Revolution. They\\ncontinue to fill factory floors and manufacturing plants, but now their\\ncapabilities extend beyond manual activities to cognitive tasks that, until\\nrecently, only humans were capable of performing. Judging song\\ncompetitions, driving automobiles, and mopping the floor with professional\\nchess players are three examples of the specific complex tasks machines are\\nnow capable of simulating.\\nBut their remarkable feats trigger fear among some observers. Part of this\\nfear nestles on the neck of survivalist insecurities, where it provokes the\\ndeep-seated question of \\nwhat if\\n? \\nWhat if\\n intelligent machines turn on us in a\\nstruggle of the fittest? \\nWhat if\\n intelligent machines produce offspring with\\ncapabilities that humans never intended to impart to machines? \\nWhat if\\n the\\nlegend of the \\nsingularity\\n is true?\\nThe other notable fear is the threat to job security, and if you’re a truck driver\\nor an accountant, there is a valid reason to be worried. According to the\\nBritish Broadcasting Company’s (BBC) interactive online resource \\nWill a\\nrobot take my job?\\n, professions such as bar worker (77%), waiter (90%),\\nchartered accountant (95%), receptionist (96%), and taxi driver (57%) each\\nhave a high chance of becoming automated by the year 2035.\\n[1]\\nBut research on planned job automation and crystal ball gazing with respect\\nto the future evolution of machines and artificial intelligence (AI) should be\\nread with a pinch of skepticism. AI technology is moving fast, but broad\\nadoption is still an unchartered path fraught with known and unforeseen\\nchallenges. Delays and other obstacles are inevitable.\\nNor is machine learning a simple case of flicking a switch and asking the\\nmachine to predict the outcome of the Super Bowl and serve you a delicious\\nmartini. Machine learning is far from what you would call an out-of-the-box\\nsolution.\\nMachines operate based on statistical algorithms managed and overseen by\\nskilled individuals—known as \\ndata scientists\\n and \\nmachine learning\\nengineers\\n. This is one labor market where job opportunities are destined for', metadata={'source': 'ml.pdf', 'page': 4}),\n",
       " Document(page_content='growth but where, currently, supply is struggling to meet demand. Industry\\nexperts lament that one of the biggest obstacles delaying the progress of AI is\\nthe inadequate supply of professionals with the necessary expertise and\\ntraining.\\nAccording to Charles Green, the Director of Thought Leadership at Belatrix\\nSoftware:\\n“It’s a huge challenge to find data scientists, people with machine\\nlearning experience, or people with the skills to analyze and use the\\ndata, as well as those who can create the algorithms required for\\nmachine learning. Secondly, while the technology is still emerging, there\\nare many ongoing developments. It’s clear that AI is a long way from\\nhow we might imagine it.”\\n \\n[2]\\nPerhaps your own path to becoming an expert in the field of machine learning\\nstarts here, or maybe a baseline understanding is sufficient to satisfy your\\ncuriosity for now. In any case, let’s proceed with the assumption that you are\\nreceptive to the idea of training to become a successful data scientist or\\nmachine learning engineer.\\nTo build and program intelligent machines, you must first understand\\nclassical statistics. Algorithms derived from classical statistics contribute the\\nmetaphorical blood cells and oxygen that power machine learning. Layer\\nupon layer of linear regression, \\nk\\n-nearest neighbors, and random forests surge\\nthrough the machine and drive their cognitive abilities. Classical statistics is\\nat the heart of machine learning and many of these algorithms are based on\\nthe same statistical equations you studied in high school. Indeed, statistical\\nalgorithms were conducted on paper well before machines ever took on the\\ntitle of \\nartificial intelligence\\n.\\nComputer programming is another indispensable part of machine learning.\\nThere isn’t a click-and-drag or Web 2.0 solution to perform advanced\\nmachine learning in the way one can conveniently build a website nowadays\\nwith WordPress or Strikingly. Programming skills are therefore vital to\\nmanage data and design statistical models that run on machines.\\nSome students of machine learning will have years of programming\\nexperience but haven’t touched classical statistics since high school. Others,\\nperhaps, never even attempted statistics in their high school years. But not to\\nworry, many of the machine learning algorithms we discuss in this book have\\nworking implementations in your programming language of choice; no\\nequation writing necessary. You can use code to execute the actual number', metadata={'source': 'ml.pdf', 'page': 5}),\n",
       " Document(page_content='crunching for you.\\nIf you have not learned to code before, you will need to if you wish to make\\nfurther progress in this field. But for the purpose of this compact starter’s\\ncourse, the curriculum can be completed without any background in\\ncomputer programming. This book focuses on the high-level fundamentals of\\nmachine learning as well as the mathematical and statistical underpinnings of\\ndesigning machine learning models.\\nFor those who do wish to look at the programming aspect of machine\\nlearning, Chapter 13 walks you through the entire process of setting up a\\nsupervised learning model using the popular programming language Python.', metadata={'source': 'ml.pdf', 'page': 6}),\n",
       " Document(page_content='WHAT IS MACHINE LEARNING?\\nIn 1959, IBM published a paper in the \\nIBM Journal of Research and\\nDevelopment\\n with an, at the time, obscure and curious title. Authored by\\nIBM’s Arthur Samuel, the paper invested the use of machine learning in the\\ngame of checkers “to verify the fact that a computer can be programmed so\\nthat it will learn to play a better game of checkers than can be played by the\\nperson who wrote the program.”\\n \\n[3]\\nAlthough it was not the first publication to use the term “machine learning”\\nper se, Arthur Samuel is widely considered as the first person to coin and\\ndefine machine learning in the form we now know today.\\n \\nSamuel’s landmark\\njournal submission, \\nSome Studies in Machine Learning Using the Game of\\nCheckers, \\nis also an early indication of homo sapiens’ determination to\\nimpart our own system of learning to man-made machines.\\n \\nFigure 1: Historical mentions of “machine learning” in published books.\\n \\nSource: Google Ngram Viewer, 2017\\n \\nArthur Samuel introduces\\n \\nmachine learning in his paper as a subfield of\\ncomputer science that gives computers the ability to learn without being\\nexplicitly programmed.\\n \\n[4]\\n Almost six decades later, this definition remains\\nwidely accepted.\\nAlthough not directly mentioned in Arthur Samuel’s definition, a key feature\\nof machine learning is the concept of \\nself-learning. \\nThis refers to the\\napplication of statistical modeling to detect patterns and improve', metadata={'source': 'ml.pdf', 'page': 7}),\n",
       " Document(page_content='performance based on data and empirical information; all without direct\\nprogramming commands. This is what Arthur Samuel described as the ability\\nto learn without being explicitly programmed. But he doesn’t infer that\\nmachines formulate decisions with no upfront programming. On the contrary,\\nmachine learning is heavily dependent on computer programming. Instead,\\nSamuel observed that machines don’t require a direct \\ninput command\\n to\\nperform a set task but rather \\ninput data\\n.\\n \\nFigure 2: Comparison of Input Command vs Input Data\\n \\nAn example of an input command is typing “2+2” into a programming\\nlanguage such as Python and hitting “Enter.”\\n>>> 2+2\\n4\\n>>>\\nThis represents a direct command with a direct answer.\\nInput data, however, is different. Data is fed to the machine, an algorithm is\\nselected, hyperparameters (settings) are configured and adjusted, and the\\nmachine is instructed to conduct its analysis. The machine proceeds to\\ndecipher patterns found in the data through the process of trial and error. The\\nmachine’s data model, formed from analyzing data patterns, can then be used\\nto predict future values.\\nAlthough there is a relationship between the programmer and the machine,\\nthey operate a layer apart in comparison to traditional computer\\nprogramming. This is because the machine is formulating decisions based on\\nexperience and mimicking the process of human-based decision-making.\\nAs an example, let’s say that after examining the YouTube viewing habits of\\ndata scientists your machine identifies a strong relationship between data', metadata={'source': 'ml.pdf', 'page': 8}),\n",
       " Document(page_content='scientists and cat videos. Later, your machine identifies patterns among the\\nphysical traits of baseball players and their likelihood of winning the season’s\\nMost Valuable Player (MVP) award. In the first scenario, the machine\\nanalyzed what videos data scientists enjoy watching on YouTube based on\\nuser engagement; measured in likes, subscribes, and repeat viewing. In the\\nsecond scenario, the machine assessed the physical features of previous\\nbaseball MVPs among various other features such as age and education.\\nHowever, in neither of these two scenarios was your machine explicitly\\nprogrammed to produce a direct outcome. You fed the input data and\\nconfigured the nominated algorithms, but the final prediction was determined\\nby the machine through self-learning and data modeling.\\nYou can think of building a data model as similar to training a guide dog.\\nThrough specialized training, guide dogs learn how to respond in various\\nsituations. For example, the dog will learn to heel at a red light or to safely\\nlead its master around obstacles. If the dog has been properly trained, then,\\neventually, the trainer will no longer be required; the guide dog will be able\\nto apply its training in various unsupervised situations. Similarly, machine\\nlearning models can be trained to form decisions based on past experience.\\nA simple example is creating a model that detects spam email messages. The\\nmodel is trained to block emails with suspicious subject lines and body text\\ncontaining three or more flagged keywords: dear friend, free, invoice, PayPal,\\nViagra, casino, payment, bankruptcy, and winner. At this stage, though, we\\nare not yet performing machine learning. If we recall the visual representation\\nof \\ninput command vs input data\\n, we can see that this process consists of only\\ntwo steps: Command > Action.\\nMachine learning entails a three-step process: Data > Model > Action.\\nThus, to incorporate machine learning into our spam detection system, we\\nneed to switch out “command” for “data” and add “model” in order to\\nproduce an action (output). In this example, the data comprises sample emails\\nand the model consists of statistical-based rules. The parameters of the model\\ninclude the same keywords from our original negative list. The model is then\\ntrained and tested against the data.\\nOnce the data is fed into the model, there is a strong chance that assumptions\\ncontained in the model will lead to some inaccurate predictions. For example,\\nunder the rules of this model, the following email subject line would\\nautomatically be classified as spam: “\\nPayPal\\n has received your \\npayment\\n for\\nCasino\\n Royale purchased on eBay.”', metadata={'source': 'ml.pdf', 'page': 9}),\n",
       " Document(page_content='As this is a genuine email sent from a PayPal auto-responder, the spam\\ndetection system is lured into producing a false positive based on the negative\\nlist of keywords contained in the model. Traditional programming is highly\\nsusceptible to such cases because there is no built-in mechanism to test\\nassumptions and modify the rules of the model. Machine learning, on the\\nother hand, can adapt and modify assumptions through its three-step process\\nand by reacting to errors.\\n \\nTraining & Test Data\\nIn machine learning, data is split into \\ntraining data\\n and \\ntest data\\n. The first\\nsplit of data, i.e. the initial reserve of data you use to develop your\\n \\nmodel,\\nprovides the training data. In the spam email detection example, false\\npositives similar to the PayPal auto-response might be detected from the\\ntraining data. New rules or modifications must then be added, e.g., email\\nnotifications issued from the sending address “payments@paypal.com”\\nshould be excluded from spam filtering.\\nAfter you have successfully developed a model based on the training data and\\nare satisfied with its accuracy, you can then test the model on the remaining\\ndata, known as the test data. Once you are satisfied with the results of both\\nthe training data and test data, the machine learning model is ready to filter\\nincoming emails and generate decisions on how to categorize those incoming\\nmessages.\\nThe difference between machine learning and traditional programming may\\nseem trivial at first, but it will become clear as you run through further\\nexamples and witness the special power of self-learning in more nuanced\\nsituations.\\nThe second important point to take away from this chapter is how machine\\nlearning fits into the broader landscape of data science and computer science.\\nThis means understanding how machine learning interrelates with parent\\nfields and sister disciplines. This is important, as you will encounter these\\nrelated terms when searching for relevant study materials—and you will hear\\nthem mentioned ad nauseam in introductory machine learning courses.\\nRelevant disciplines can also be difficult to tell apart at first glance, such as\\n“machine learning” and “data mining.”\\nLet’s begin with a high-level introduction. Machine learning, data mining,\\ncomputer programming, and most relevant fields (excluding classical', metadata={'source': 'ml.pdf', 'page': 10}),\n",
       " Document(page_content='statistics) derive first from computer science, which encompasses everything\\nrelated to the design and use of computers. Within the all-encompassing\\nspace of computer science is the next broad field: data science. Narrower than\\ncomputer science, data science comprises methods and systems to extract\\nknowledge and insights from data through the use of computers.\\n \\nFigure 3: The lineage of machine learning represented by a row of Russian matryoshka dolls\\n \\nPopping out from computer science and data science as the third matryoshka\\ndoll is artificial intelligence. Artificial intelligence, or AI, encompasses the\\nability of machines to perform intelligent and cognitive tasks. Comparable to\\nthe way the Industrial Revolution gave birth to an era of machines that could\\nsimulate physical tasks, AI is driving the development of machines capable\\nof simulating cognitive abilities.\\nWhile still broad but dramatically more honed than computer science and\\ndata science, AI contains numerous subfields that are popular today. These\\nsubfields include search and planning, reasoning and knowledge\\nrepresentation, perception, natural language processing (NLP), and of course,\\nmachine learning. Machine learning bleeds into other fields of AI, including\\nNLP and perception through the shared use of self-learning algorithms.', metadata={'source': 'ml.pdf', 'page': 11}),\n",
       " Document(page_content='Figure 4: Visual representation of the relationship between data-related fields\\n \\nFor students with an interest in AI, machine learning provides an excellent\\nstarting point in that it offers a more narrow and practical lens of study\\ncompared to the conceptual ambiguity of AI. Algorithms found in machine\\nlearning can also be applied across other disciplines, including perception and\\nnatural language processing. In addition, a Master’s degree is adequate to\\ndevelop a certain level of expertise in machine learning, but you may need a\\nPhD to make any true progress in AI.\\nAs mentioned, machine learning also overlaps with data mining—a sister\\ndiscipline that focuses on discovering and unearthing patterns in large\\ndatasets. Popular algorithms, such as \\nk\\n-means clustering, association analysis,\\nand regression analysis, are applied in both data mining and machine learning\\nto analyze data. But where machine learning focuses on the incremental\\nprocess of self-learning and data modeling to form predictions about the\\nfuture, data mining narrows in on cleaning large datasets to glean valuable\\ninsight from the past.\\nThe difference between data mining and machine learning can be explained\\nthrough an analogy of two teams of archaeologists. The first team is made up\\nof archaeologists who focus their efforts on removing debris that lies in the\\nway of valuable items, hiding them from direct sight. Their primary goals are\\nto excavate the area, find new valuable discoveries, and then pack up their\\nequipment and move on. A day later, they will fly to another exotic\\ndestination to start a new project with no relationship to the site they', metadata={'source': 'ml.pdf', 'page': 12}),\n",
       " Document(page_content='excavated the day before.\\nThe second team is also in the business of excavating historical sites, but\\nthese archaeologists use a different methodology. They deliberately reframe\\nfrom excavating the main pit for several weeks. In that time, they visit other\\nrelevant archaeological sites in the area and examine how each site was\\nexcavated. After returning to the site of their own project, they apply this\\nknowledge to excavate smaller pits surrounding the main pit.\\nThe archaeologists then analyze the results. After reflecting on their\\nexperience excavating one pit, they optimize their efforts to excavate the\\nnext. This includes predicting the amount of time it takes to excavate a pit,\\nunderstanding variance and patterns found in the local terrain and developing\\nnew strategies to reduce error and improve the accuracy of their work. From\\nthis experience, they are able to optimize their approach to form a strategic\\nmodel to excavate the main pit.\\nIf it is not already clear, the first team subscribes to data mining and the\\nsecond team to machine learning. At a micro-level, both data mining and\\nmachine learning appear similar, and they do use many of the same tools.\\nBoth teams make a living excavating historical sites to discover valuable\\nitems. But in practice, their methodology is different. The machine learning\\nteam focuses on dividing their dataset into training data and test data to create\\na model, and improving future predictions based on previous experience.\\nMeanwhile, the data mining team concentrates on excavating the target area\\nas effectively as possible—without the use of a self-learning model—before\\nmoving on to the next cleanup job.', metadata={'source': 'ml.pdf', 'page': 13}),\n",
       " Document(page_content='ML CATEGORIES\\nMachine learning incorporates several hundred statistical-based algorithms\\nand choosing the right algorithm or combination of algorithms for the job is a\\nconstant challenge for anyone working in this field. But before we examine\\nspecific algorithms, it is important to understand the three overarching\\ncategories of machine learning. These three categories are \\nsupervised\\n,\\nunsupervised,\\n and \\nreinforcement\\n.\\n \\nSupervised Learning\\nAs the first branch of machine learning, supervised learning concentrates on\\nlearning patterns through connecting the relationship between variables and\\nknown outcomes and working with labeled datasets.\\nSupervised learning works by feeding the machine sample data with various\\nfeatures (represented as “X”) and the correct value output of the data\\n(represented as “y”). The fact that the output and feature values are known\\nqualifies the dataset as “labeled.” The algorithm then deciphers patterns that\\nexist in the data and creates a model that can reproduce the same underlying\\nrules with new data.\\nFor instance, to predict the market rate for the purchase of a used car, a\\nsupervised algorithm can formulate predictions by analyzing the relationship\\nbetween car attributes (including the year of make, car brand, mileage, etc.)\\nand the selling price of other cars sold based on historical data. Given that the\\nsupervised algorithm knows the final price of other cards sold, it can then\\nwork backward to determine the relationship between the characteristics of\\nthe car and its value.', metadata={'source': 'ml.pdf', 'page': 14}),\n",
       " Document(page_content='Figure 1: Car value prediction model\\n \\nAfter the machine deciphers the rules and patterns of the data, it creates what\\nis known as a model: an algorithmic equation for producing an outcome with\\nnew data based on the rules derived from the training data. Once the model is\\nprepared, it can be applied to new data and tested for accuracy. After the\\nmodel has passed both the training and test data stages, it is ready to be\\napplied and used in the real world.\\nIn Chapter 13, we will create a model for predicting house values where y is\\nthe actual house price and X are the variables that impact y, such as land size,\\nlocation, and the number of rooms. Through supervised learning, we will\\ncreate a rule to predict y (house value) based on the given values of various\\nvariables (X).\\nExamples of supervised learning algorithms include regression analysis,\\ndecision trees, \\nk\\n-nearest neighbors, neural networks, and support vector\\nmachines. Each of these techniques will be introduced later in the book.\\n \\nUnsupervised Learning\\nIn the case of unsupervised learning, not all variables and data patterns are\\nclassified. Instead, the machine must uncover hidden patterns and create\\nlabels through the use of unsupervised learning algorithms. The\\n k\\n-means\\nclustering algorithm is a popular example of unsupervised learning.\\n \\nThis\\nsimple algorithm groups data points that are found to possess similar features\\nas shown in Figure 1.', metadata={'source': 'ml.pdf', 'page': 15}),\n",
       " Document(page_content='Figure 1: Example of \\nk\\n-means clustering, a popular unsupervised learning technique\\n \\nIf you group data points based on the purchasing behavior of SME (Small\\nand Medium-sized Enterprises) and large enterprise customers, for example,\\nyou are likely to see two clusters emerge. This is because SMEs and large\\nenterprises tend to have disparate buying habits. When it comes to purchasing\\ncloud infrastructure, for instance, basic cloud hosting resources and a Content\\nDelivery Network (CDN) may prove sufficient for most SME customers.\\nLarge enterprise customers, though, are more likely to purchase a wider array\\nof cloud products and entire solutions that include advanced security and\\nnetworking products like WAF (Web Application Firewall), a dedicated\\nprivate connection, and VPC (Virtual Private Cloud). By analyzing customer\\npurchasing habits, unsupervised learning is capable of identifying these two\\ngroups of customers without specific labels that classify the company as\\nsmall, medium or large.\\nThe advantage of unsupervised learning is it enables you to discover patterns\\nin the data that you were unaware existed—such as the presence of two major\\ncustomer types. Clustering techniques such as \\nk\\n-means clustering can also\\nprovide the springboard for conducting further analysis after discrete groups\\nhave been discovered.\\nIn industry, unsupervised learning is particularly powerful in fraud detection\\n—where the most dangerous attacks are often those yet to be classified. One\\nreal-world example is DataVisor, who essentially built their business model\\nbased on unsupervised learning.\\nFounded in 2013 in California, DataVisor protects customers from fraudulent', metadata={'source': 'ml.pdf', 'page': 16}),\n",
       " Document(page_content='online activities, including spam, fake reviews, fake app installs, and\\nfraudulent transactions. Whereas traditional fraud protection services draw on\\nsupervised learning models and rule engines, DataVisor uses unsupervised\\nlearning which enables them to detect unclassified categories of attacks in\\ntheir early stages.\\nOn their website, DataVisor explains that \"to detect attacks, existing solutions\\nrely on human experience to create rules or labeled training data to tune\\nmodels. This means they are unable to detect new attacks that haven’t already\\nbeen identified by humans or labeled in training data.\"\\n \\n[5]\\nThis means that traditional solutions analyze the chain of activity for a\\nparticular attack and then create rules to predict a repeat attack. Under this\\nscenario, the dependent variable (y) is the event of an attack and the\\nindependent variables (X) are the common predictor variables of an attack.\\nExamples of independent variables could be:\\na) A sudden large order from an unknown user.\\n I.E. established customers\\ngenerally spend less than $100 per order, but a new user spends $8,000 in one\\norder immediately upon registering their account.\\nb) A sudden surge of user ratings.\\n I.E. As a typical author and bookseller\\non Amazon.com, it’s uncommon for my first published work to receive more\\nthan one book review within the space of one to two days. In general,\\napproximately 1 in 200 Amazon readers leave a book review and most books\\ngo weeks or months without a review. However, I commonly see competitors\\nin this category (data science) attracting 20-50 reviews in one day!\\n(Unsurprisingly, I also see Amazon removing these suspicious reviews weeks\\nor months later.)\\nc) Identical or similar user reviews from different users.\\n Following the\\nsame Amazon analogy, I often see user reviews of my book appear on other\\nbooks several months later (sometimes with a reference to my name as the\\nauthor still included in the review!). Again, Amazon eventually removes\\nthese fake reviews and suspends these accounts for breaking their terms of\\nservice.\\nd) Suspicious shipping address.\\n I.E. For small businesses that routinely ship\\nproducts to local customers, an order from a distant location (where they\\ndon\\'t advertise their products) can in rare cases be an indicator of fraudulent\\nor malicious activity.\\nStandalone activities such as a sudden large order or a distant shipping\\naddress may prove too little information to predict sophisticated', metadata={'source': 'ml.pdf', 'page': 17}),\n",
       " Document(page_content='cybercriminal activity and more likely to lead to many false positives. But a\\nmodel that monitors combinations of independent variables, such as a sudden\\nlarge purchase order from the other side of the globe or a landslide of book\\nreviews that reuse existing content will generally lead to more accurate\\npredictions. A supervised learning-based model could deconstruct and\\nclassify what these common independent variables are and design a detection\\nsystem to identify and prevent repeat offenses.\\nSophisticated cybercriminals, though, learn to evade classification-based rule\\nengines by modifying their tactics. In addition, leading up to an attack,\\nattackers often register and operate single or multiple accounts and incubate\\nthese accounts with activities that mimic legitimate users. They then utilize\\ntheir established account history to evade detection systems, which are\\ntrigger-heavy against recently registered accounts. Supervised learning-based\\nsolutions struggle to detect sleeper cells until the actual damage has been\\nmade and especially with regard to new categories of attacks.\\nDataVisor and other anti-fraud solution providers therefore leverage\\nunsupervised learning to address the limitations of supervised learning by\\nanalyzing patterns across hundreds of millions of accounts and identifying\\nsuspicious connections between users—without knowing the actual category\\nof future attacks. By grouping malicious actors and analyzing their\\nconnections to other accounts, they are able to prevent new types of attacks\\nwhose independent variables are still unlabeled and unclassified. Sleeper cells\\nin their incubation stage (mimicking legitimate users) are also identified\\nthrough their association to malicious accounts. Clustering algorithms such as\\nk\\n-means clustering can generate these groupings without a full training\\ndataset in the form of independent variables that clearly label indications of\\nan attack, such as the four examples listed earlier. Knowledge of the\\ndependent variable (known attackers) is generally the key to identifying other\\nattackers before the next attack occurs. The other plus side of unsupervised\\nlearning is companies like DataVisor can uncover entire criminal rings by\\nidentifying subtle correlations across users.\\nWe will cover unsupervised learning later in this book specific to clustering\\nanalysis. Other examples of unsupervised learning include association\\nanalysis, social network analysis, and descending dimension algorithms.\\n \\nReinforcement Learning\\nReinforcement learning is the third and most advanced algorithm category in', metadata={'source': 'ml.pdf', 'page': 18}),\n",
       " Document(page_content='machine learning. Unlike supervised and unsupervised learning,\\nreinforcement learning continuously improves its model by leveraging\\nfeedback from previous iterations. This is different to supervised and\\nunsupervised learning, which both reach an indefinite endpoint after a model\\nis formulated from the training and test data segments.\\nReinforcement learning can be complicated and is probably best explained\\nthrough an analogy to a video game. As a player progresses through the\\nvirtual space of a game, they learn the value of various actions under different\\nconditions and become more familiar with the field of play. Those learned\\nvalues then inform and influence a player’s subsequent behavior and their\\nperformance immediately improves based on their learning and past\\nexperience.\\nReinforcement learning is very similar, where algorithms are set to train the\\nmodel through continuous learning. A standard reinforcement learning model\\nhas measurable performance criteria where outputs are not tagged—instead,\\nthey are graded. In the case of self-driving vehicles, avoiding a crash will\\nallocate a positive score and in the case of chess, avoiding defeat will\\nlikewise receive a positive score.\\nA specific algorithmic example of reinforcement learning is Q-learning. In Q-\\nlearning, you start with a set environment of \\nstates, \\nrepresented by the\\nsymbol ‘S’. In the game Pac-Man, states could be the challenges, obstacles or\\npathways that exist in the game. There may exist a wall to the left, a ghost to\\nthe right, and a power pill above—each representing different \\nstates\\n.\\nThe set of possible actions to respond to these states is referred to as “A.” In\\nthe case of Pac-Man, actions are limited to left, right, up, and down\\nmovements, as well as multiple combinations thereof.\\nThe third important symbol is “Q.” Q is the starting value and has an initial\\nvalue of “0.”\\nAs Pac-Man explores the space inside the game, two main things will\\nhappen:\\n1) Q drops as negative things occur after a given state/action\\n2) Q increases as positive things occur after a given state/action\\nIn Q-learning, the machine will learn to match the action for a given state that\\ngenerates or maintains the highest level of Q. It will learn initially through the\\nprocess of random movements (actions) under different conditions (states).\\nThe machine will record its results (rewards and penalties) and how they\\nimpact its Q level and store those values to inform and optimize its future', metadata={'source': 'ml.pdf', 'page': 19}),\n",
       " Document(page_content='actions.\\nWhile this sounds simple enough, implementation is a much more difficult\\ntask and beyond the scope of an absolute beginner’s introduction to machine\\nlearning. Reinforcement learning algorithms aren’t covered in this book,\\nhowever, I will leave you with a link to a more comprehensive explanation of\\nreinforcement learning and Q-learning following the Pac-Man scenario.\\nhttps://inst.eecs.berkeley.edu/~cs188/sp12/projects/reinforcement/reinforcement.html', metadata={'source': 'ml.pdf', 'page': 20}),\n",
       " Document(page_content='THE ML TOOLBOX\\nA handy way to learn a new subject area is to map and visualize the essential\\nmaterials and tools inside a toolbox.\\nIf you were packing a toolbox to build websites, for example, you would first\\npack a selection of programming languages. This would include frontend\\nlanguages such as HTML, CSS, and JavaScript, one or two backend\\nprogramming languages based on personal preferences, and of course, a text\\neditor. You might throw in a website builder such as WordPress and then\\nhave another compartment filled with web hosting, DNS, and maybe a few\\ndomain names that you’ve recently purchased.\\nThis is not an extensive inventory, but from this general list, you can start to\\ngain a better appreciation of what tools you need to master in order to\\nbecome a successful website developer.\\nLet’s now unpack the toolbox for machine learning.\\n \\nCompartment 1: Data\\nIn the first compartment is your data. Data constitutes the input variables\\nneeded to form a prediction. Data comes in many forms, including structured\\nand non-structured data. As a beginner, it is recommended that you start with\\nstructured data\\n. This means that the data is defined and labeled (with\\nschema) in a table, as shown here:', metadata={'source': 'ml.pdf', 'page': 21}),\n",
       " Document(page_content='Before we proceed, I first want to explain the anatomy of a tabular dataset. A\\ntabular (table-based) dataset contains data organized in rows and columns. In\\neach column is a \\nfeature\\n. A feature is also known as a \\nvariable, \\na\\n dimension\\nor an \\nattribute—\\nbut they all mean the same thing.\\nEach individual row represents a single observation of a given\\nfeature/variable. Rows are sometimes referred to as a \\ncase\\n or \\nvalue\\n, but in\\nthis book, we will use the term “row.”\\n \\nFigure 1: Example of a tabular dataset\\n \\nEach column is known as a \\nvector\\n. Vectors store your X and y values and\\nmultiple vectors (columns) are commonly referred to as \\nmatrices\\n. In the case\\nof supervised learning, y will already exist in your dataset and be used to\\nidentify patterns in relation to independent variables (X). The y values are\\ncommonly expressed in the final column, as shown in Figure 2.', metadata={'source': 'ml.pdf', 'page': 22}),\n",
       " Document(page_content='Figure 2: The y value is often but not always expressed in the far right column\\n \\nNext, within the first compartment of the toolbox is a range of scatterplots,\\nincluding 2-D, 3-D, and 4-D plots. A 2-D scatterplot consists of a vertical\\naxis (known as the y-axis) and a horizontal axis (known as the x-axis) and\\nprovides the graphical canvas to plot a series of dots, known as data points.\\nEach data point on the scatterplot represents one observation from the dataset,\\nwith X values plotted on the x-axis and y values plotted on the y-axis.', metadata={'source': 'ml.pdf', 'page': 23}),\n",
       " Document(page_content='Figure 3: Example of a 2-D scatterplot. X represents days passed since the recording of Bitcoin prices and y represents recorded Bitcoin price.\\n \\nCompartment 2: Infrastructure\\nThe second compartment of the toolbox contains your infrastructure, which\\nconsists of platforms and tools to process data. As a beginner to machine\\nlearning, you are likely to be using a web application (such as Jupyter\\nNotebook) and a programming language like Python. There are then a series\\nof machine learning libraries, including NumPy, Pandas, and Scikit-learn that\\nare compatible with Python. Machine learning libraries are a collection of\\npre-compiled programming routines frequently used in machine learning.\\nYou will also need a machine from which to work, in the form of a computer\\nor a virtual server. In addition, you may need specialized libraries for data\\nvisualization such as Seaborn and Matplotlib, or a standalone software\\nprogram like Tableau, which supports a range of visualization\\ntechniques including charts, graphs, maps, and other visual options.\\nWith your infrastructure sprayed out across the table (hypothetically of\\ncourse), you are now ready to get to work building your first machine\\nlearning model. The first step is to crank up your computer. Laptops and\\ndesktop computers are both suitable for working with smaller datasets. You\\nwill then need to install a programming environment, such as Jupyter\\nNotebook, and a programming language, which for most beginners is Python.\\nPython is the most widely used programming language for machine learning\\nbecause:\\na)\\n     \\nIt is easy to learn and operate,\\nb)\\n     \\nIt is compatible with a range of machine learning libraries, and\\nc)\\n     \\nIt can be used for related tasks, including data collection (web\\nscraping) and data piping (Hadoop and Spark).\\nOther go-to languages for machine learning include C and C++. If you’re\\nproficient with C and C++ then it makes sense to stick with what you already', metadata={'source': 'ml.pdf', 'page': 24}),\n",
       " Document(page_content='know. C and C++ are the default programming languages for advanced\\nmachine learning because they can run directly on a GPU (Graphical\\nProcessing Unit). Python needs to be converted first before it can run on a\\nGPU, but we will get to this and what a GPU is later in the chapter.\\nNext, Python users will typically install the following libraries: NumPy,\\nPandas, and Scikit-learn. NumPy is a free and open-source library that allows\\nyou to efficiently load and work with large datasets, including managing\\nmatrices.\\nScikit-learn provides access to a range of popular algorithms, including linear\\nregression, Bayes’ classifier, and support vector machines.\\nFinally, Pandas enables your data to be represented on a virtual\\nspreadsheet that you can control through code. It shares many of the same\\nfeatures as Microsoft Excel in that it allows you to edit data and perform\\ncalculations. In fact, the name Pandas derives from the term “panel data,”\\nwhich refers to its ability to create a series of panels, similar to “sheets” in\\nExcel. Pandas is also ideal for importing and extracting data from CSV files.\\n \\nFigure 4: Previewing a table in Jupyter Notebook using Pandas\\n \\nIn summary, users can draw on these three libraries to:\\n1) Load and work with a dataset via NumPy.\\n2) Clean up and perform calculations on data, and extract data from CSV files\\nwith Pandas.\\n3) Implement algorithms with Scikit-learn.\\nFor students seeking alternative programming options (beyond Python, C,\\nand C++), other relevant programming languages for machine learning\\ninclude R, MATLAB, and Octave.\\nR is a free and open-source programming language optimized for', metadata={'source': 'ml.pdf', 'page': 25}),\n",
       " Document(page_content='mathematical operations, and conducive to building matrices and statistical\\nfunctions, which are built directly into the language libraries of R. Although\\nR is commonly used for data analytics and data mining, R supports machine\\nlearning operations as well.\\nMATLAB and Octave are direct competitors to R. MATLAB is a commercial\\nand propriety programming language. It is strong in regards to solving\\nalgebraic equations and is also a quick programming language to learn.\\nMATLAB is widely used in electrical engineering, chemical engineering,\\ncivil engineering, and aeronautical engineering. However, computer scientists\\nand computer engineers tend not to rely on MATLAB as heavily and\\nespecially in recent times. In machine learning, MATLAB is more often used\\nin academia than in industry. Thus, while you may see MATLAB featured in\\nonline courses, and especially on Coursera, this is not to say that it’s\\ncommonly used in the wild. If, however, you’re coming from an engineering\\nbackground, MATLAB is certainly a logical choice.\\nLastly, Octave is essentially a free version of MATLAB developed in\\nresponse to MATLAB by the open-source community.\\n \\nCompartment 3: Algorithms\\nNow that the machine learning environment is set up and you’ve chosen your\\nprogramming language and libraries, you can next import your data directly\\nfrom a CSV file. You can find hundreds of interesting datasets in CSV format\\nfrom kaggle.com. After registering as a member of their platform, you can\\ndownload a dataset of your choice. Best of all, Kaggle datasets are free and\\nthere is no cost to register as a user.\\nThe dataset will download directly to your computer as a CSV file, which\\nmeans you can use Microsoft Excel to open and even perform basic\\nalgorithms such as linear regression on your dataset.\\nNext is the third and final compartment that stores the algorithms. Beginners\\nwill typically start off by using simple supervised learning algorithms such as\\nlinear regression, logistic regression, decision trees, and \\nk\\n-nearest neighbors.\\nBeginners are also likely to apply unsupervised learning in the form of \\nk\\n-\\nmeans clustering and descending dimension algorithms.\\n \\nVisualization\\nNo matter how impactful and insightful your data discoveries are, you need a', metadata={'source': 'ml.pdf', 'page': 26}),\n",
       " Document(page_content='way to effectively communicate the results to relevant decision-makers. This\\nis where data visualization, a highly effective medium to communicate data\\nfindings to a general audience, comes in handy. The visual message conveyed\\nthrough graphs, scatterplots, box plots, and the representation of numbers in\\nshapes makes for quick and easy storytelling.\\nIn general, the less informed your audience is, the more important it is to\\nvisualize your findings. Conversely, if your audience is knowledgeable about\\nthe topic, additional details and technical terms can be used to supplement\\nvisual elements.\\nTo visualize your results you can draw on Tableau or a Python library such as\\nSeaborn, which are stored in the second compartment of the toolbox.', metadata={'source': 'ml.pdf', 'page': 27}),\n",
       " Document(page_content='Advanced Toolbox\\nWe have so far examined the toolbox for a typical beginner, but what about\\nan advanced user? What would their toolbox look like? While it may take\\nsome time before you get to work with the advanced toolkit, it doesn’t hurt to\\nhave a sneak peek.\\nThe toolbox for an advanced learner resembles the beginner’s toolbox but\\nnaturally comes with a broader spectrum of tools and, of course, data. One of\\nthe biggest differences between a beginner and an advanced learner is the size\\nof the data they manage and operate. Beginners naturally start by working\\nwith small datasets that are easy to manage and which can be downloaded\\ndirectly to one’s desktop as a simple CSV file. Advanced learners, though,\\nwill be eager to tackle massive datasets, well in the vicinity of big data.\\n \\nCompartment 1: Big Data\\nBig data is used to describe a dataset that, due to its value, variety, volume,\\nand velocity, defies conventional methods of processing and would be\\nimpossible for a human to process without the assistance of an advanced\\nmachine. Big data does not have an exact definition in terms of size or the\\ntotal number of rows and columns. At the moment, petabytes qualify as big\\ndata, but datasets are becoming increasingly larger as we find new ways to\\nefficiently collect and store data at low cost. And with big data also comes\\ngreater noise and complicated data structures. A huge part, therefore, of\\nworking with big data is \\nscrubbing\\n: the process of refining your dataset\\nbefore building your model, which will be covered in the next chapter.\\n \\nCompartment 2: Infrastructure\\nAfter scrubbing the dataset, the next step is to pull out your machine learning\\nequipment. In terms of tools, there are no real surprises. Advanced learners\\nare still using the same machine learning libraries, programming languages,\\nand programming environments as beginners.\\nHowever, given that advanced learners are now dealing with up to petabytes\\nof data, robust infrastructure is required. Instead of relying on the CPU of a\\npersonal computer, advanced students typically turn to distributed computing\\nand a cloud provider such as Amazon Web Services (AWS) to run their data\\nprocessing on what is known as a Graphical Processing Unit (GPU) instance.', metadata={'source': 'ml.pdf', 'page': 28}),\n",
       " Document(page_content='GPU chips were originally added to PC motherboards and video consoles\\nsuch as the PlayStation 2 and the Xbox for gaming purposes. They were\\ndeveloped to accelerate the creation of images with millions of pixels whose\\nframes needed to be constantly recalculated to display output in less than a\\nsecond. By 2005, GPU chips were produced in such large quantities that their\\nprice had dropped dramatically and they’d essentially matured into a\\ncommodity. Although highly popular in the video game industry, the\\napplication of such computer chips in the space of machine learning was not\\nfully understood or realized until recently.\\nIn his 2016 novel, \\nThe Inevitable: Understanding the 12 Technological\\nForces That Will Shape Our Future\\n, Founding Executive Editor of Wired\\nMagazine, Kevin Kelly, explains that in 2009, Andrew Ng and a team at\\nStanford University discovered how to link inexpensive GPU clusters to run\\nneural networks consisting of hundreds of millions of node connections.\\n“Traditional processors required several weeks to calculate all the cascading\\npossibilities in a neural net with one hundred million parameters. Ng found\\nthat a cluster of GPUs could accomplish the same thing in a day.”\\n \\n[6]\\nAs a specialized parallel computing chip, GPU instances are able to perform\\nmany more floating point operations per second than a CPU, allowing for\\nmuch faster solutions with linear algebra and statistics than with a CPU.\\nIt is important to note that C and C++ are the preferred languages to directly\\nedit and perform mathematical operations on the GPU. However, Python can\\nalso be used and converted into C in combination with TensorFlow from\\nGoogle.\\nAlthough it’s possible to run TensorFlow on the CPU, you can gain up to\\nabout 1,000x in performance using the GPU. Unfortunately for Mac users,\\nTensorFlow is only compatible with the Nvidia GPU card, which is no longer\\navailable with Mac OS X. Mac users can still run TensorFlow on their CPU\\nbut will need to engineer a patch/external driver or run their workload on the\\ncloud to access GPU. Amazon Web Services, Microsoft Azure, Alibaba\\nCloud, Google Cloud Platform, and other cloud providers offer pay-as-you-\\ngo GPU resources, which may start off free through a free trial program.\\nGoogle Cloud Platform is currently regarded as a leading option for GPU\\nresources based on performance and pricing. In 2016, Google also announced\\nthat it would publicly release a Tensor Processing Unit designed specifically\\nfor running TensorFlow, which is already used internally at Google.', metadata={'source': 'ml.pdf', 'page': 29}),\n",
       " Document(page_content='Compartment 3: Advanced Algorithms\\nTo round out this chapter, let’s have a look at the third compartment of the\\nadvanced toolbox containing machine learning algorithms.\\nTo analyze large datasets, advanced learners work with a plethora of\\nadvanced algorithms including Markov models, support vector machines, and\\nQ-learning, as well as a series of simple algorithms like those found in the\\nbeginner’s toolbox. But the algorithm family they’re most likely to use is\\nneural networks (introduced in Chapter 10), which comes with its own\\nselection of advanced machine learning libraries.\\nWhile Scikit-learn offers a range of popular shallow algorithms, TensorFlow\\nis the machine learning library of choice for deep learning/neural networks as\\nit supports numerous advanced techniques including automatic calculus for\\nback-propagation/gradient descent. Due to the depth of resources,\\ndocumentation, and jobs available with TensorFlow, it is the obvious\\nframework to learn today.\\nPopular alternative neural network libraries include Torch, Caffe, and the\\nfast-growing Keras. Written in Python, Keras is an open-source deep learning\\nlibrary that runs on top of TensorFlow, Theano, and other frameworks, and\\nallows users to perform fast experimentation in fewer lines of code. Like a\\nWordPress website theme, Keras is minimal, modular, and quick to get up\\nand running but is less flexible compared with TensorFlow and other\\nlibraries. Users will sometimes utilize Keras to validate their model before\\nswitching to TensorFlow to build a more customized model.\\nCaffe is also open-source and commonly used to develop deep learning\\narchitectures for image classification and image segmentation. Caffe is\\nwritten in C++ but has a Python interface that also supports GPU-based\\nacceleration using the Nvidia CuDNN.\\nReleased in 2002, Torch is well established in the deep learning community.\\nIt is open-source and based on the programming language Lua. Torch offers a\\nrange of algorithms for deep learning and is used within Facebook, Google,\\nTwitter, NYU, IDIAP, Purdue as well as other companies and research labs.\\n \\n[7]\\nUntil recently, Theano was another competitor to TensorFlow but as of late\\n2017, contributions to the framework have officially ceased.\\nSometimes used beside neural networks is another advanced approach called\\nensemble modeling. This technique essentially combines algorithms and\\nstatistical techniques to create a unified model, which we will explore further\\nin Chapter 12.', metadata={'source': 'ml.pdf', 'page': 30}),\n",
       " Document(page_content='DATA SCRUBBING\\nMuch like many categories of fruit, datasets nearly always require some form\\nof upfront cleaning and human manipulation before they are ready to digest.\\nFor machine learning and data science more broadly, there are a vast number\\nof techniques to scrub data.\\nScrubbing is the technical process of refining your dataset to make it more\\nworkable. This can involve modifying and sometimes removing incomplete,\\nincorrectly formatted, irrelevant or duplicated data. It can also entail\\nconverting text-based data to numerical values and the redesigning of\\nfeatures. For data practitioners, data scrubbing usually demands the greatest\\napplication of time and effort.\\n \\nFeature Selection\\nTo generate the best results from your data, it is important to first identify the\\nvariables most relevant to your hypothesis. In practice, this means being\\nselective about the variables you select to design your model.\\nRather than creating a four-dimensional scatterplot with four features in the\\nmodel, an opportunity may present to select two highly relevant features and\\nbuild a two-dimensional plot that is easier to interpret. Moreover, preserving\\nfeatures that do not correlate strongly with the outcome value can, in fact,\\nmanipulate and derail the model’s accuracy. Consider the following table\\nexcerpt downloaded from kaggle.com documenting dying languages.', metadata={'source': 'ml.pdf', 'page': 31}),\n",
       " Document(page_content='Database:\\n https://www.kaggle.com/the-guardian/extinct-languages\\n \\nLet’s say our goal is to identify variables that lead to a language becoming\\nendangered. Based on this goal, it’s unlikely that a language’s “Name in\\nSpanish\\n”\\n will lead to any relevant insight. We can therefore go ahead and\\ndelete this vector (column) from the dataset. This will help to prevent over-\\ncomplication and potential inaccuracies, and will also improve the overall\\nprocessing speed of the model.\\nSecondly, the dataset holds duplicate information in the form of separate\\nvectors for “Countries” and “Country Code.” Including both of these vectors\\ndoesn’t provide any additional insight; hence, we can choose to delete one', metadata={'source': 'ml.pdf', 'page': 32}),\n",
       " Document(page_content='and retain the other.\\nAnother method to reduce the number of features is to roll multiple features\\ninto one. In the next table, we have a list of products sold on an e-commerce\\nplatform. The dataset comprises four buyers and eight products. This is not a\\nlarge sample size of buyers and products—due in part to the spatial\\nlimitations of the book format. A real-life e-commerce platform would have\\nmany more columns to work with, but let’s go ahead with this example.\\n \\n \\nIn order to analyze the data in a more efficient way, we can reduce the\\nnumber of columns by merging similar features into fewer columns. For\\ninstance, we can remove individual product names and replace the eight\\nproduct items with a lower number of categories or subtypes. As all product\\nitems fall under the single category of “fitness,” we will sort by product\\nsubtype and compress the columns from eight to three. The three newly\\ncreated product subtype columns are “Health Food,” “Apparel,” and\\n“Digital.”\\n \\n \\nThis enables us to transform the dataset in a way that preserves and captures\\ninformation using fewer variables. The downside to this transformation is that\\nwe have less information about relationships between specific products.', metadata={'source': 'ml.pdf', 'page': 33}),\n",
       " Document(page_content='Rather than recommending products to users according to other individual\\nproducts, recommendations will instead be based on relationships between\\nproduct subtypes.\\nNonetheless, this approach does uphold a high level of data relevancy.\\nBuyers will be recommended health food when they buy other health food or\\nwhen they buy apparel (depending on the level of correlation), and obviously\\nnot machine learning textbooks—unless it turns out that there is a strong\\ncorrelation there! But alas, such a variable is outside the frame of this dataset.\\nRemember that data reduction is also a business decision, and business\\nowners in counsel with the data science team will need to consider the trade-\\noff between convenience and the overall precision of the model.\\n \\nRow Compression\\nIn addition to feature selection, there may also be an opportunity to reduce\\nthe number of rows and thereby compress the total number of data points.\\nThis can involve merging two or more rows into one. For example, in the\\nfollowing dataset, “Tiger” and “Lion” can be merged and renamed\\n“Carnivore.”\\n \\n \\nHowever, by merging these two rows (Tiger & Lion), the feature values for', metadata={'source': 'ml.pdf', 'page': 34}),\n",
       " Document(page_content='both rows must also be aggregated and recorded in a single row. In this case,\\nit is viable to merge the two rows because they both possess the same\\ncategorical values for all features except y (Race Time)—which can be\\naggregated. The race time of the Tiger and the Lion can be added and divided\\nby two.\\nNumerical values, such as time, are normally simple to aggregate unless they\\nare categorical. For instance, it would be impossible to aggregate an animal\\nwith four legs and an animal with two legs! We obviously can’t merge these\\ntwo animals and set “three” as the aggregate number of legs.\\nRow compression can also be difficult to implement when numerical values\\naren’t available. For example, the values “Japan” and “Argentina” are very\\ndifficult to merge. The countries “Japan” and “South Korea” can be merged,\\nas they can be categorized as the same continent, “Asia” or “East Asia.”\\nHowever, if we add “Pakistan” and “Indonesia” to the same group, we may\\nbegin to see skewed results, as there are significant cultural, religious,\\neconomic, and other dissimilarities between these four countries.\\nIn summary, non-numerical and categorical row values can be problematic to\\nmerge while preserving the true value of the original data. Also, row\\ncompression is normally less attainable than feature compression for most\\ndatasets.\\n \\nOne-hot Encoding\\nAfter choosing variables and rows, you next want to look for text-based\\nfeatures that can be converted into numbers. Aside from set text-based values\\nsuch as True/False (that automatically convert to “1” and “0” respectively),\\nmany algorithms and also scatterplots are not compatible with non-numerical\\ndata.\\nOne means to convert text-based features into numerical values is through\\none-hot encoding\\n, which transforms features into binary form, represented as\\n“1” or “0”—“True” or “False.” A “0,” representing False, means that the\\nfeature does not belong to a particular category, whereas a “1”—True or\\n“hot”—denotes that the feature does belong to a set category.\\nBelow is another excerpt of the dataset on dying languages, which we can use\\nto practice one-hot encoding.', metadata={'source': 'ml.pdf', 'page': 35}),\n",
       " Document(page_content='First, note that the values contained in the “No. of Speakers” column do not\\ncontain commas or spaces, e.g. 7,500,000 and 7 500 000. Although such\\nformatting does make large numbers clearer for our eyes, programming\\nlanguages don’t require such niceties. In fact, formatting numbers can lead to\\nan invalid syntax or trigger an unwanted result, depending on the\\nprogramming language you use. So remember to keep numbers unformatted\\nfor programming purposes. Feel free, though, to add spacing or commas at\\nthe data visualization stage, as this will make it easier for your audience to\\ninterpret!\\nOn the right-hand-side of the table is a vector categorizing the degree of\\nendangerment of the nine different languages. This column we can convert to\\nnumerical values by applying the one-hot encoding method, as demonstrated\\nin the subsequent table.', metadata={'source': 'ml.pdf', 'page': 36}),\n",
       " Document(page_content='Using one-hot encoding, the dataset has expanded to five columns and we\\nhave created three new features from the original feature (Degree of\\nEndangerment). We have also set each column value to “1” or “0,”\\ndepending on the original category value.\\nThis now makes it possible for us to input the data into our model and choose\\nfrom a wider array of machine learning algorithms. The downside is that we\\nhave more dataset features, which may lead to slightly longer processing\\ntime. This is nonetheless manageable, but it can be problematic for datasets\\nwhere original features are split into a larger number of new features.\\nOne hack to minimize the number of features is to restrict binary cases to a\\nsingle column. As an example, there is a speed dating dataset on kaggle.com\\nthat lists “Gender” in a single column using one-hot encoding. Rather than\\ncreate discrete columns for both “Male” and “Female,” they merged these\\ntwo features into one. According to the dataset’s key, females are denoted as\\n“0” and males are denoted as “1.” The creator of the dataset also used this\\ntechnique for “Same Race” and “Match.”', metadata={'source': 'ml.pdf', 'page': 37}),\n",
       " Document(page_content='Database:\\n https://www.kaggle.com/annavictoria/speed-dating-experiment\\n \\nBinning\\nBinning is another method of feature engineering that is used to convert\\nnumerical values into a category.\\nWhoa, hold on! Didn’t you say that numerical values were a good thing? Yes,\\nnumerical values tend to be preferred in most cases. Where numerical values\\nare less ideal, is in situations where they list variations irrelevant to the goals\\nof your analysis. Let’s take house price evaluation as an example. The exact\\nmeasurements of a tennis court might not matter greatly when evaluating\\nhouse prices. The relevant information is whether the house \\nhas\\n a tennis\\ncourt. The same logic probably also applies to the garage and the swimming\\npool, where the existence or non-existence of the variable is more influential\\nthan their specific measurements.\\nThe solution here is to replace the numeric measurements of the tennis court\\nwith a True/False feature or a categorical value such as “small,” “medium,”\\nand “large.” Another alternative would be to apply one-hot encoding with “0”\\nfor homes that \\ndo not\\n have a tennis court and “1” for homes that \\ndo\\n have a', metadata={'source': 'ml.pdf', 'page': 38}),\n",
       " Document(page_content='tennis court.\\n \\nMissing Data\\nDealing with missing data is never a desired situation. Imagine unpacking a\\njigsaw puzzle that you discover has five percent of its pieces missing.\\nMissing values in a dataset can be equally frustrating and will ultimately\\ninterfere with your analysis and final predictions. There are, however,\\nstrategies to minimize the negative impact of missing data.\\nOne approach is to approximate missing values using the \\nmode\\n value. The\\nmode represents the single most common variable value available in the\\ndataset. This works best with categorical and binary variable types.\\nFigure 1: A visual example of the mode and median respectively\\n \\nThe second approach to manage missing data is to approximate missing\\nvalues using the \\nmedian\\n value, which adopts the value(s) located in the\\nmiddle of the dataset. This works best with integers (whole numbers) and\\ncontinuous variables (numbers with decimals).\\nAs a last resort, rows with missing values can be removed altogether. The\\nobvious downside to this approach is having less data to analyze and\\npotentially less comprehensive results.', metadata={'source': 'ml.pdf', 'page': 39}),\n",
       " Document(page_content='SETTING UP YOUR DATA\\nOnce you have cleaned your dataset, the next job is to split the data into two\\nsegments for testing and training. It is very important not to test your model\\nwith the same data that you used for training. The ratio of the two splits\\nshould be approximately 70/30 or 80/20. This means that your training data\\nshould account for 70 percent to 80 percent of the rows in your dataset, and\\nthe other 20 percent to 30 percent of rows is your test data. It is vital to split\\nyour data by rows and not columns.\\n \\nFigure 1: Training and test partitioning of the dataset 70/30\\n \\nBefore you split your data, it is important that you randomize all rows in the\\ndataset. This helps to avoid bias in your model, as your original dataset might\\nbe arranged sequentially depending on the time it was collected or some other\\nfactor. Unless you randomize your data, you may accidentally omit important\\nvariance from the training data that will cause unwanted surprises when you', metadata={'source': 'ml.pdf', 'page': 40}),\n",
       " Document(page_content=\"apply the trained model to your test data. Fortunately, Scikit-learn provides a\\nbuilt-in function to shuffle and randomize your data with just one line of code\\n(demonstrated in Chapter 13).\\nAfter randomizing your data, you can begin to design your model and apply\\nthat to the training data. The remaining 30 percent or so of data is put to the\\nside and reserved for testing the accuracy of the model.\\nIn the case of supervised learning, the model is developed by feeding the\\nmachine the training data and the expected output (y). The machine is able to\\nanalyze and discern relationships between the features (X) found in the\\ntraining data to calculate the final output (y).\\nThe next step is to measure how well the model actually performs. A\\ncommon approach to analyzing prediction accuracy is a measure called \\nmean\\nabsolute error\\n, which examines each prediction in the model and provides an\\naverage error score for each prediction.\\nIn Scikit-learn, mean absolute error is found using the model.predict function\\non X (features). This works by first plugging in the y values from the training\\ndataset and generating a prediction for each row in the dataset. Scikit-learn\\nwill compare the predictions of the model to the correct outcome and measure\\nits accuracy. You will know if your model is accurate when the error rate\\nbetween the training and test dataset is low. This means that the model has\\nlearned the dataset’s underlying patterns and trends.\\nOnce the model can adequately predict the values of the test data, it is ready\\nfor use in the wild. If the model fails to accurately predict values from the test\\ndata, you will need to check whether the training and test data were properly\\nrandomized. Alternatively, you may need to change the model's\\nhyperparameters.\\nEach algorithm has hyperparameters; these are your algorithm settings. In\\nsimple terms, these settings control and impact how fast the model learns\\npatterns and which patterns to identify and analyze.\\n \\nCross Validation\\nAlthough the training/test data split can be effective in developing models\\nfrom existing data, a question mark remains as to whether the model will\\nwork on new data. If your existing dataset is too small to construct an\\naccurate model, or if the training/test partition of data is not appropriate, this\\ncan lead to poor estimations of performance in the wild.\", metadata={'source': 'ml.pdf', 'page': 41}),\n",
       " Document(page_content='Fortunately, there is an effective workaround for this issue. Rather than\\nsplitting the data into two segments (one for training and one for testing), we\\ncan implement what is known as \\ncross validation\\n. Cross validation\\nmaximizes the availability of training data by splitting data into various\\ncombinations and testing each specific combination.\\nCross validation can be performed through two primary methods. The first\\nmethod is \\nexhaustive cross validation\\n, which involves finding and testing all\\npossible combinations to divide the original sample into a training set and a\\ntest set. The alternative and more common method is non-exhaustive cross\\nvalidation, known as \\nk-fold validation\\n. The \\nk\\n-fold validation technique\\ninvolves splitting data into \\nk\\n assigned buckets and reserving one of those\\nbuckets to test the training model at each round.\\nTo perform \\nk\\n-fold validation, data are first randomly assigned to \\nk\\n number of\\nequal sized buckets. One bucket is then reserved as the test bucket and is used\\nto measure and evaluate the performance of the remaining (\\nk\\n-1) buckets.\\n \\nFigure 2: \\nk\\n-fold validation\\n \\nThe cross validation process is repeated \\nk\\n number of times (“folds”). At each\\nfold, one bucket is reserved to test the training model generated by the other\\nbuckets. The process is repeated until all buckets have been utilized as both a', metadata={'source': 'ml.pdf', 'page': 42}),\n",
       " Document(page_content='training and test bucket. The results are then aggregated and combined to\\nformulate a single model.\\nBy using all available data for both training and testing purposes, the \\nk\\n-fold\\nvalidation technique\\n \\ndramatically minimizes potential error (such as\\noverfitting) found by relying on a fixed split of training and test data.\\n \\nHow Much Data Do I Need?\\nA common question for students starting out in machine learning \\nis how\\nmuch data do I need to train my dataset? \\nIn general, machine learning works\\nbest when your training dataset includes a full range of feature combinations.\\nWhat does a full range of feature combinations look like? Imagine you have a\\ndataset about data scientists categorized by the following features:\\n- University degree (X)\\n- 5+ years professional experience (X)\\n- Children (X)\\n- Salary (y)\\nTo assess the relationship that the first three features (X) have to a data\\nscientist’s salary (y), we need a dataset that includes the y value for each\\ncombination of features. For instance, we need to know the salary for data\\nscientists with a university degree, 5+ years professional experience and that\\ndon’t have children, as well as data scientists with a university degree, 5+\\nyears professional experience and that do have children.\\nThe more available combinations, the more effective the model will be at\\ncapturing how each attribute affects y (the data scientist’s salary). This will\\nensure that when it comes to putting the model into practice on the test data\\nor real-life data, it won’t immediately unravel at the sight of unseen\\ncombinations.\\nAt a minimum, a machine learning model should typically have ten times as\\nmany data points as the total number of features. So for a small dataset with\\nthree features, the training data should ideally have at least thirty rows.\\nThe other point to remember is that more relevant data is usually better than\\nless. Having more relevant data allows you to cover more combinations and\\ngenerally helps to ensure more accurate predictions. In some cases, it might\\nnot be possible or cost-effective to source data for every possible\\ncombination. In these cases, you will need to make do with the data that you\\nhave at your disposal.', metadata={'source': 'ml.pdf', 'page': 43}),\n",
       " Document(page_content='The following chapters will examine specific algorithms commonly used in\\nmachine learning. Please note that I include some equations out of necessity,\\nand I have tried to keep them as simple as possible. Many of the machine\\nlearning techniques that we discuss in this book already have working\\nimplementations in your programming language of choice—no equation\\nwriting necessary.', metadata={'source': 'ml.pdf', 'page': 44}),\n",
       " Document(page_content=\"REGRESSION ANALYSIS\\nAs the “Hello World” of machine learning algorithms, regression analysis is\\na simple supervised learning technique used to find the best trendline to\\ndescribe a dataset.\\nThe first regression analysis technique that we will examine is linear\\nregression, which uses a straight line to describe a dataset. To unpack this\\nsimple technique, let’s return to the earlier dataset charting Bitcoin values to\\nthe US Dollar.\\n \\n \\nImagine you’re back in high school and it's the year 2015 (which is probably\\nmuch more recent than your actual year of graduation!). During your senior\\nyear, a news headline piques your interest in Bitcoin. With your natural\\ntendency to chase the next shiny object, you tell your family about your\\ncryptocurrency aspirations. But before you have a chance to bid for your first\\nBitcoin on Coinbase, your father intervenes and insists that you try paper\\ntrading before you go risking your life savings. “Paper trading” is using\\nsimulated means to buy and sell an investment without involving actual\\nmoney.\\nSo over the next twenty-four months, you track the value of Bitcoin and write\\ndown its value at regular intervals. You also keep a tally of how many days\\nhave passed since you first started paper trading. You never anticipated to\\nstill be paper trading after two years, but unfortunately, you never got a\", metadata={'source': 'ml.pdf', 'page': 45}),\n",
       " Document(page_content='chance to enter the cryptocurrency market. As suggested by your father, you\\nwaited for the value of Bitcoin to drop to a level you could afford. But\\ninstead, the value of Bitcoin exploded in the opposite direction.\\nNonetheless, you haven’t lost hope of one day owning Bitcoin. To assist your\\ndecision on whether you continue to wait for the value to drop or to find an\\nalternative investment class, you turn your attention to statistical analysis.\\nYou first reach into your toolbox for a scatterplot. With the blank scatterplot\\nin your hands, you proceed to plug in your x and y coordinates from your\\ndataset and plot Bitcoin values from 2015 to 2017. However, rather than use\\nall three columns from the table, you select the second (Bitcoin price) and\\nthird (No. of Days Transpired) columns to build your model and populate the\\nscatterplot (shown in Figure 1). As we know, numerical values (found in the\\nsecond and third columns) are easy to plug into a scatterplot and require no\\nspecial conversion or one-hot encoding. What’s more, the first and third\\ncolumns contain the same variable of “time” and the third column alone is\\nsufficient.\\n \\nFigure 1: Bitcoin values from 2015-2017 plotted on a scatterplot\\n \\nAs your goal is to estimate what Bitcoin will be valued at in the future, the y-\\naxis plots the dependent variable, which is “Bitcoin Price.” The independent\\nvariable (X), in this case, is time. The “No. of Days Transpired” is thereby\\nplotted on the x-axis.', metadata={'source': 'ml.pdf', 'page': 46}),\n",
       " Document(page_content='After plotting the x and y values on the scatterplot, you can immediately see a\\ntrend in the form of a curve ascending from left to right with a steep increase\\nbetween day 607 and day 736. Based on the upward trajectory of the curve, it\\nmight be time to quit hoping for a drop in value.\\nHowever, an idea suddenly pops up into your head. What if instead of\\nwaiting for the value of Bitcoin to fall to a level that you can afford, you\\ninstead borrow from a friend and purchase Bitcoin now at day 736? Then,\\nwhen the value of Bitcoin rises further, you can pay back your friend and\\ncontinue to earn asset appreciation on the Bitcoin you fully own.\\nIn order to assess whether it’s worth borrowing from your friend, you will\\nneed to first estimate how much you can earn in potential profit. Then you\\nneed to figure out whether the return on investment will be adequate to pay\\nback your friend in the short-term.\\nIt’s now time to reach into the third compartment of the toolbox for an\\nalgorithm. One of the simplest algorithms in machine learning is regression\\nanalysis, which is used to determine the strength of a relationship between\\nvariables. Regression analysis comes in many forms, including linear, non-\\nlinear, logistic, and multilinear, but let’s take a look first at linear regression.\\nLinear regression comprises a straight line that splits your data points on a\\nscatterplot. The goal of linear regression is to split your data in a way that\\nminimizes the distance between the regression line and all data points on the\\nscatterplot. This means that if you were to draw a vertical line from the\\nregression line to each data point on the graph, the aggregate distance of each\\npoint would equate to the smallest possible distance to the regression line.', metadata={'source': 'ml.pdf', 'page': 47}),\n",
       " Document(page_content='Figure 2: Linear regression line\\n \\nThe regression line is plotted on the scatterplot in Figure 2. The technical\\nterm for the regression line is the \\nhyperplane\\n, and you will see this term used\\nthroughout your study of machine learning. A hyperplane is practically a\\ntrendline—and this is precisely how Google Sheets titles linear regression in\\nits scatterplot customization menu.\\nAnother important feature of regression is \\nslope\\n, which can be conveniently\\ncalculated by referencing the hyperplane. As one variable increases, the other\\nvariable will increase at the average value denoted by the hyperplane. The\\nslope is therefore very useful in formulating predictions. For example, if you\\nwish to estimate the value of Bitcoin at 800 days, you can enter 800 as your x\\ncoordinate and reference the slope by finding the corresponding y value\\nrepresented on the hyperplane. In this case, the y value is USD $1,850.', metadata={'source': 'ml.pdf', 'page': 48}),\n",
       " Document(page_content='Figure 3: The value of Bitcoin at day 800\\n \\nAs shown in Figure 3, the hyperplane reveals that you actually stand to lose\\nmoney on your investment at day 800 (after buying on day 736)! Based on\\nthe slope of the hyperplane, Bitcoin is expected to depreciate in value\\nbetween day 736 and day 800—despite no precedent in your dataset for\\nBitcoin ever dropping in value.\\nWhile it’s needless to say that linear regression isn’t a fail-proof method to\\npicking investment trends, the trendline does offer a basic reference point to\\npredict the future. If we were to use the trendline as a reference point earlier\\nin time, say at day 240, then the prediction posted would have been more\\naccurate. At day 240 there is a low degree of deviation from the hyperplane,\\nwhile at day 736 there is a high degree of deviation. Deviation refers to the\\ndistance between the hyperplane and the data point.', metadata={'source': 'ml.pdf', 'page': 49}),\n",
       " Document(page_content='Figure 4: The distance of the data points to the hyperplane\\n \\nIn general, the closer the data points are to the regression line, the more\\naccurate the final prediction. If there is a high degree of deviation between\\nthe data points and the regression line, the slope will provide less accurate\\npredictions. Basing your predictions on the data point at day 736, where there\\nis high deviation, results in poor accuracy. In fact, the data point at day 736\\nconstitutes an outlier because it does not follow the same general trend as the\\nprevious four data points. What’s more, as an outlier it exaggerates the\\ntrajectory of the hyperplane based on its high y-axis value. Unless future data\\npoints scale in proportion to the y-axis values of the outlier data point, the\\nmodel’s predictive accuracy will suffer.\\n \\nCalculation Example\\nAlthough your programming language will take care of this automatically,\\nit’s useful to understand how linear regression is actually calculated. We will\\nuse the following dataset and formula to perform linear regression.', metadata={'source': 'ml.pdf', 'page': 50}),\n",
       " Document(page_content='# The final two columns of the table are not part of the original dataset and have been added for convenience to complete the following equation.\\nWhere:\\nΣ = Total sum\\nΣx = Total sum of all x values (1 + 2 + 1 + 4 + 3 = 11)\\nΣy = Total sum of all y values (3 + 4 + 2 + 7 + 5 = 21)\\nΣxy = Total sum of x*y for each row (3 + 8 + 2 + 28 + 15 = 56)\\nΣx\\n2 \\n= Total sum of x*x for each row (1 + 4 + 1 + 16 + 9 = 31)\\nn = Total number of rows. In the case of this example, n = 5.', metadata={'source': 'ml.pdf', 'page': 51}),\n",
       " Document(page_content='A =\\n((21 x 31) – (11 x 56)) / (5(31) – 11\\n2\\n)\\n(651 – 616) / (155 – 121)\\n35 / 34\\n1.029\\n \\nB =\\n(5(56) – (11 x 21)) / (5(31) – 11\\n2\\n)\\n(280 – 231) / (155 – 121)\\n49 / 34\\n1.44\\nInsert the “a” and “b” values into a linear equation.\\ny = a + bx\\ny = 1.029 + 1.441x\\nThe linear equation y = 1.029 + 1.441x dictates how to draw the hyperplane.', metadata={'source': 'ml.pdf', 'page': 52}),\n",
       " Document(page_content='Figure 5: The linear regression hyperplane plotted on the scatterplot\\n \\nLet’s now test the regression line by looking up the coordinates for x = 2.\\ny = 1.029 + 1.441(x)\\ny = 1.029 + 1.441(2)\\ny = 3.911\\nIn this case, the prediction is very close to the actual result of 4.0.\\n \\nLogistic Regression\\nA large part of data analysis boils down to a simple question: is something\\n“A” or “B?” Is it “positive” or “negative?” Is this person a “potential\\ncustomer” or “not a potential customer?” Machine learning accommodates\\nsuch questions through logistic equations, and specifically through what is\\nknown as the \\nsigmoid function\\n. The sigmoid function produces an S-shaped\\ncurve that can convert any number and map it into a numerical value between\\n0 and 1, but it does so without ever reaching those exact limits.\\nA common application of the sigmoid function is found in logistic regression.\\nLogistic regression adopts the sigmoid function to analyze data and predict\\ndiscrete classes that exist in a dataset. Although logistic regression shares a\\nvisual resemblance to linear regression, it is technically a classification\\ntechnique. Whereas linear regression addresses numerical equations and\\nforms numerical predictions to discern relationships between variables,', metadata={'source': 'ml.pdf', 'page': 53}),\n",
       " Document(page_content=\"logistic regression predicts discrete classes.\\n \\nFigure 6: An example of logistic regression\\n \\nLogistic regression is typically used for binary classification to predict two\\ndiscrete classes, e.g. \\npregnant\\n or \\nnot pregnant\\n. To do this, the sigmoid\\nfunction (shown as follows) is added to compute the result and convert\\nnumerical results into an expression of probability between 0 and 1.\\nThe logistic sigmoid function above is calculated as “1” divided by “1” plus\\n“e” raised to the power of negative “x,” where:\\nx = the numerical value you wish to transform\\ne = Euler's constant, 2.718\\nIn a binary case, a value of 0 represents no chance of occurring, and 1\\nrepresents a certain chance of occurring. The degree of probability for values\\nlocated between 0 and 1 can be calculated according to how close they rest to\\n0 (impossible) or 1 (certain possibility) on the scatterplot.\", metadata={'source': 'ml.pdf', 'page': 54}),\n",
       " Document(page_content='Figure 7: A sigmoid function used to classify data points\\n \\nBased on the found probabilities we can assign each data point to one of two\\ndiscrete classes. As seen in Figure 7, we can create a cut-off point at 0.5 to\\nclassify the data points into classes. Data points that record a value above 0.5\\nare classified as Class A, and any data points below 0.5 are classified as Class\\nB. Data points that record a result of exactly 0.5 are unclassifiable, but such\\ninstances are rare due to the mathematical component of the sigmoid\\nfunction.\\nPlease also note that this formula alone does not produce the hyperplane\\ndividing discrete categories as seen earlier in Figure 6. The statistical formula\\nfor plotting the logistic hyperplane is somewhat more complicated and can be\\nconveniently plotted using your programming language.\\nGiven its strength in binary classification, logistic regression is used in many\\nfields including fraud detection, disease diagnosis, emergency detection, loan\\ndefault detection, or to identify spam email through the process of identifying\\nspecific classes, e.g. non-spam and spam. However, logistic regression can\\nalso be applied to ordinal cases where there are a set number of discrete\\nvalues, e.g. single, married, and divorced.\\nLogistic regression with more than two outcome values is known as', metadata={'source': 'ml.pdf', 'page': 55}),\n",
       " Document(page_content='multinomial logistic regression, which can be seen in Figure 8.\\n \\nFigure 8: An example of multinomial logistic regression\\n \\nTwo tips to remember when performing logistic regression are that the data\\nshould be free of missing values and that all variables are independent of\\neach other. There should also be sufficient data for each outcome value to\\nensure high accuracy. A good starting point would be approximately 30-50\\ndata points for each outcome, i.e. 60-100 total data points for binary logistic\\nregression.\\n \\nSupport Vector Machine\\nAs an advanced category of regression, support vector machine (SVM)\\nresembles logistic regression but with stricter conditions. To that end, SVM is\\nsuperior at drawing classification boundary lines. Let’s examine what this\\nlooks like in action.', metadata={'source': 'ml.pdf', 'page': 56}),\n",
       " Document(page_content='Figure 9: Logistic regression versus SVM\\n \\nThe scatterplot in Figure 9 consists of data points that are linearly separable\\nand the logistic hyperplane (A) splits the data points into two classes in a way\\nthat minimizes the distance between all data points and the hyperplane. The\\nsecond line, the SVM hyperplane (B), likewise separates the two clusters, but\\nfrom a position of maximum distance between itself and the two clusters.\\nYou will also notice a gray area that denotes \\nmargin\\n, which is the distance\\nbetween the hyperplane and the nearest data point, multiplied by two. The\\nmargin is a key feature of SVM and is important because it offers additional\\nsupport to cope with new data points that may infringe on a logistic\\nregression hyperplane. To illustrate this scenario, let’s consider the same\\nscatterplot with the inclusion of a new data point.', metadata={'source': 'ml.pdf', 'page': 57}),\n",
       " Document(page_content='Figure 10: A new data point is added to the scatterplot\\n \\nThe new data point is a circle, but it is located incorrectly on the left side of\\nthe logistic regression hyperplane (designated for stars). The new data point,\\nthough, remains correctly located on the right side of the SVM hyperplane\\n(designated for circles) courtesy of ample “support” supplied by the margin.', metadata={'source': 'ml.pdf', 'page': 58}),\n",
       " Document(page_content='Figure 11: Mitigating anomalies\\n \\nAnother useful application case of SVM is for mitigating anomalies. A\\nlimitation of standard logistic regression is that it goes out of its way to fit\\nanomalies (as seen in the scatterplot with the star in the bottom right corner in\\nFigure 11). SVM, however, is less sensitive to such data points and actually\\nminimizes their impact on the final location of the boundary line. In Figure\\n11, we can see that Line B (SVM hyperplane) is less sensitive to the\\nanomalous star on the right-hand side. SVM can thus be used as one method\\nto fight anomalies.\\nThe examples seen so far have comprised two features plotted on a two-\\ndimensional scatterplot. However, SVM’s real strength is found in high-\\ndimensional data and handling multiple features. SVM has numerous\\nvariations available to classify high-dimensional data, known as “kernels,”\\nincluding linear SVC (seen in Figure 12), polynomial SVC, and the Kernel\\nTrick. The Kernel Trick is an advanced solution to map data from a low-\\ndimensional to a high-dimensional space. Transitioning from a two-\\ndimensional to a three-dimensional space allows you to use a linear plane to\\nsplit the data within a 3-D space, as seen in Figure 12.', metadata={'source': 'ml.pdf', 'page': 59}),\n",
       " Document(page_content='Figure 12: Example of linear SVC', metadata={'source': 'ml.pdf', 'page': 60}),\n",
       " Document(page_content='CLUSTERING\\nOne helpful approach to analyze information is to identify clusters of data\\nthat share similar attributes. For example, your company may wish to\\nexamine a segment of customers that purchase at the same time of the year\\nand discern what factors influence their purchasing behavior.\\nBy understanding a particular cluster of customers, you can form decisions\\nabout which products to recommend to customer groups through promotions\\nand personalized offers. Outside of market research, clustering can be applied\\nto various other scenarios, including pattern recognition, fraud detection,\\nand image processing.\\nClustering analysis falls under the banner of both supervised learning and\\nunsupervised learning. As a supervised learning technique, clustering is used\\nto classify new data points into existing clusters through \\nk\\n-nearest neighbors\\n(\\nk\\n-NN) and as an unsupervised learning technique, clustering is applied to\\nidentify discrete groups of data points through \\nk\\n-means clustering. Although\\nthere are other forms of clustering techniques, these two algorithms are\\ngenerally the most popular in both machine learning and data mining.\\n \\nk\\n-Nearest Neighbors\\nThe simplest clustering algorithm is \\nk\\n-nearest neighbors (\\nk\\n-NN); a supervised\\nlearning technique used to classify new data points based on the relationship\\nto nearby data points.\\nk\\n-NN is similar to a voting system or a popularity contest. Think of it as\\nbeing the new kid in school and choosing a group of classmates to socialize\\nwith based on the five classmates who sit nearest to you. Among the five\\nclassmates, three are \\ngeeks\\n, one is a \\nskater,\\n and one is a \\njock\\n. According to\\nk\\n-NN, you would choose to hang out with the \\ngeeks\\n based on their numerical\\nadvantage. Let’s look at another example.', metadata={'source': 'ml.pdf', 'page': 61}),\n",
       " Document(page_content='Figure 1: An example of \\nk-\\nNN clustering used to predict the class of a new data point\\n \\nAs seen in Figure 1, the scatterplot enables us to compute the distance\\nbetween any two data points. The data points on the scatterplot have already\\nbeen categorized into two clusters. Next, a new data point whose class is\\nunknown is added to the plot. We can predict the category of the new data\\npoint based on its relationship to existing data points.\\nFirst though, we must set “\\nk\\n” to determine how many data points we wish to\\nnominate to classify the new data point. If we set \\nk\\n to 3, \\nk\\n-NN will only\\nanalyze the new data point’s relationship to the three closest data points\\n(neighbors). The outcome of selecting the three closest neighbors returns two\\nClass B data points and one Class A data point. Defined by \\nk\\n (3), the model’s\\nprediction for determining the category of the new data point is Class B as it\\nreturns two out of the three nearest neighbors.\\nThe chosen number of neighbors identified, defined by \\nk\\n, is crucial in\\ndetermining the results. In Figure 1, you can see that classification will\\nchange depending on whether \\nk\\n is set to “3” or “7.” It is therefore\\nrecommended that you test numerous \\nk\\n combinations to find the best fit and\\navoid setting \\nk\\n too low or too high. Setting \\nk \\nto an uneven number will also\\nhelp to eliminate the possibility of a statistical stalemate and invalid result.\\nThe default number of neighbors is five when using Scikit-learn.', metadata={'source': 'ml.pdf', 'page': 62}),\n",
       " Document(page_content='Although generally a highly accurate and simple technique to learn, storing\\nan entire dataset and calculating the distance between each new data point\\nand all existing data points does place a heavy burden on computing\\nresources. Thus, \\nk\\n-NN is generally not recommended for use with large\\ndatasets.\\nAnother potential downside is that it can be challenging to apply \\nk\\n-NN to\\nhigh-dimensional data (3-D and 4-D) with multiple features. Measuring\\nmultiple distances between data points in a three or four-dimensional space is\\ntaxing on computing resources and also complicated to perform accurate\\nclassification. Reducing the total number of dimensions, through a\\ndescending dimension algorithm such as Principle Component Analysis\\n(PCA) or merging variables, is a common strategy to simplify and prepare a\\ndataset for \\nk\\n-NN analysis.\\n \\nk\\n-Means Clustering\\nAs a popular unsupervised learning algorithm, \\nk\\n-means clustering attempts to\\ndivide data into \\nk\\n discrete groups and is effective at uncovering basic data\\npatterns. Examples of potential groupings include animal species, customers\\nwith similar features, and housing market segmentation. The\\n k\\n-means\\nclustering algorithm works by first splitting data into \\nk\\n number of clusters\\nwith \\nk\\n representing the number of clusters you wish to create. If you choose\\nto split your dataset into three clusters then \\nk\\n,\\n \\nfor example, is set to 3.\\n \\nFigure 2: Comparison of original data and clustered data using \\nk-\\nmeans', metadata={'source': 'ml.pdf', 'page': 63}),\n",
       " Document(page_content='In Figure 2, we can see that the original (unclustered) data has been\\ntransformed into three clusters (\\nk\\n is 3). If we were to set \\nk\\n to 4, an additional\\ncluster would be derived from the dataset to produce four clusters.\\nHow does \\nk\\n-means clustering separate the data points? The first step is to\\nexamine the unclustered data on the scatterplot and manually select a centroid\\nfor each \\nk\\n cluster. That centroid then forms the epicenter of an individual\\ncluster. Centroids can be chosen at random, which means you can nominate\\nany data point on the scatterplot to act as a centroid. However, you can save\\ntime by choosing centroids dispersed across the scatterplot and not directly\\nadjacent to each other. In other words, start by guessing where you think the\\ncentroids for each cluster might be located. The remaining data points on the\\nscatterplot are then assigned to the closest centroid by measuring the\\nEuclidean distance.\\n \\nFigure 3: Calculating Euclidean distance\\n \\nEach data point can be assigned to only one cluster and each cluster is\\ndiscrete. This means that there is no overlap between clusters and no case of\\nnesting a cluster inside another cluster. Also, all data points, including\\nanomalies, are assigned to a centroid irrespective of how they impact the final\\nshape of the cluster. However, due to the statistical force that pulls all nearby\\ndata points to a central point, your clusters will generally form an elliptical or\\nspherical shape.\\n \\nFigure 4: Example of an ellipse cluster', metadata={'source': 'ml.pdf', 'page': 64}),\n",
       " Document(page_content='After all data points have been allocated to a centroid, the next step is to\\naggregate the mean value of all data points for each cluster, which can be\\nfound by calculating the average x and y values of all data points in that\\ncluster.\\nNext, take the mean value of the data points in each cluster and plug in those\\nx and y values to update your centroid coordinates. This will most likely\\nresult in a change to your centroids’ location. Your total number of clusters,\\nhowever, will remain the same. You are not creating new clusters, rather\\nupdating their position on the scatterplot. Like musical chairs, the remaining\\ndata points will then rush to the closest centroid to form \\nk\\n number of clusters.\\nShould any data point on the scatterplot switch clusters with the changing of\\ncentroids, the previous step is repeated. This means, again, calculating the\\naverage mean value of the cluster and updating the x and y values of each\\ncentroid to reflect the average coordinates of the data points in that cluster.\\nOnce you reach a stage where the data points no longer switch clusters after\\nan update in centroid coordinates, the algorithm is complete, and you have\\nyour final set of clusters. The following diagrams break down the full\\nalgorithmic process.\\n \\nFigure 5: Sample data points are plotted on a scatterplot', metadata={'source': 'ml.pdf', 'page': 65}),\n",
       " Document(page_content='Figure 6: Two data points are nominated as centroids\\n \\nFigure 7: Two clusters are formed after calculating the Euclidean distance of the remaining data points to the centroids.', metadata={'source': 'ml.pdf', 'page': 66}),\n",
       " Document(page_content='Figure 8: The centroid coordinates for each cluster are updated to reflect the cluster’s mean value. As one data point has switched from the right cluster to the left cluster, the\\ncentroids of both clusters are recalculated.\\n \\nFigure 9: Two final clusters are produced based on the updated centroids for each cluster\\n \\nSetting \\nk\\nIn setting \\nk\\n, it is important to strike the right number of clusters. In general,\\nas \\nk\\n increases, clusters become smaller and variance falls. However, the\\ndownside is that neighboring clusters become less distinct from one another\\nas \\nk\\n increases.', metadata={'source': 'ml.pdf', 'page': 67}),\n",
       " Document(page_content='If you set \\nk\\n to the same number of data points in your dataset, each data point\\nautomatically converts into a standalone cluster. Conversely, if you set \\nk\\n to 1,\\nthen all data points will be deemed as homogenous and produce only one\\ncluster. Needless to say, setting \\nk\\n to either extreme will not provide any\\nworthy insight to analyze.\\n \\nFigure 10: A scree plot\\n \\nIn order to optimize \\nk\\n, you may wish to turn to a scree plot for guidance. A\\nscree plot charts the degree of scattering (variance) inside a cluster as the\\ntotal number of clusters increase. Scree plots are famous for their iconic\\n“elbow,” which reflects several pronounced kinks in the plot’s curve.\\nA scree plot compares the Sum of Squared Error (SSE) for each variation of\\ntotal clusters. SSE is measured as the sum of the squared distance between\\nthe centroid and the other neighbors inside the cluster. In a nutshell, SSE\\ndrops as more clusters are formed.\\nThis then raises the question of what the optimal number of clusters is. In\\ngeneral, you should opt for a cluster solution where SSE subsides\\ndramatically to the left on the scree plot, but before it reaches a point of\\nnegligible change with cluster variations to its right. For instance, in Figure\\n10, there is little impact on SSE for six or more clusters. This would result in\\nclusters that would be small and difficult to distinguish.\\nIn this scree plot, two or three clusters appear to be an ideal solution. There', metadata={'source': 'ml.pdf', 'page': 68}),\n",
       " Document(page_content='exists a significant kink to the left of these two cluster variations due to a\\npronounced drop-off in SSE. Meanwhile, there is still some change in SSE\\nwith the solution to their right. This will ensure that these two cluster\\nsolutions are distinct and have an impact on data classification.\\nA more simple and non-mathematical approach to setting \\nk \\nis applying\\ndomain knowledge. For example, if I am analyzing data concerning visitors\\nto the website of a major IT provider, I might want to set \\nk \\nto 2. Why two\\nclusters? Because I already know there is likely to be a major discrepancy in\\nspending behavior between returning visitors and new visitors. First-time\\nvisitors rarely purchase enterprise-level IT products and services, as these\\ncustomers will normally go through a lengthy research and vetting process\\nbefore procurement can be approved.\\nHence, I can use \\nk\\n-means clustering to create two clusters and test my\\nhypothesis. After creating two clusters, I may then want to examine one of\\nthe two clusters further, either applying another technique or again using \\nk-\\nmeans clustering\\n. \\nFor example, I might want to split returning users into two\\nclusters (using \\nk-\\nmeans clustering\\n)\\n to test my hypothesis that mobile users\\nand desktop users produce two disparate groups of data points. Again, by\\napplying domain knowledge, I know it is uncommon for large enterprises to\\nmake big-ticket purchases on a mobile device. Still, I wish to create a\\nmachine learning model to test this assumption.\\nIf, though, I am analyzing a product page for a low-cost item, such as a $4.99\\ndomain name, new visitors and returning visitors are less likely to produce\\ntwo clear clusters. As the product item is of low value, new users are less\\nlikely to deliberate before purchasing.\\nInstead, I might choose to set \\nk \\nto 3 based on my three primary lead\\ngenerators: organic traffic, paid traffic, and email marketing. These three lead\\nsources are likely to produce three discrete clusters based on the facts that:\\na)\\n     \\nOrganic traffic\\n generally consists of both new and returning\\ncustomers with a strong intent of purchasing from my website (through\\npre-selection, e.g. word of mouth, previous customer experience).\\nb)\\n     \\nPaid traffic\\n targets new customers who typically arrive on the\\nwebsite with a lower level of trust than organic traffic, including\\npotential customers who click on the paid advertisement by mistake.\\nc)\\n     \\nEmail marketing\\n reaches existing customers who already have\\nexperience purchasing from the website and have established user\\naccounts.', metadata={'source': 'ml.pdf', 'page': 69}),\n",
       " Document(page_content='This is an example of domain knowledge based on my own occupation, but\\ndo understand that the effectiveness of “domain knowledge” diminishes\\ndramatically past a low number of \\nk\\n clusters. In other words, domain\\nknowledge might be sufficient for determining two to four clusters, but it will\\nbe less valuable in choosing between 20 or 21 clusters.', metadata={'source': 'ml.pdf', 'page': 70}),\n",
       " Document(page_content='BIAS & VARIANCE\\nAlgorithm selection is an important step in forming an accurate prediction\\nmodel, but deploying an algorithm with a high rate of accuracy can be a\\ndifficult balancing act. The fact that each algorithm can produce vastly\\ndifferent models based on the hyperparameters provided can lead to\\ndramatically different results. As mentioned earlier, hyperparameters are the\\nalgorithm’s settings, similar to the controls on the dashboard of an airplane or\\nthe knobs used to tune radio frequency—except hyperparameters are lines of\\ncode!\\n \\nFigure 1: Example of hyperparameters in Python for the algorithm gradient boosting\\n \\nA constant challenge in machine learning is navigating \\nunderfitting\\n and\\noverfitting\\n, which describe how closely your model follows the actual\\npatterns of the dataset. To understand underfitting and overfitting, you must\\nfirst understand \\nbias\\n and \\nvariance\\n.\\nBias refers to the gap between your predicted value and the actual value. In\\nthe case of high bias, your predictions are likely to be skewed in a certain\\ndirection away from the actual values. Variance describes how scattered your\\npredicted values are. Bias and variance can be best understood by analyzing\\nthe following visual representation.', metadata={'source': 'ml.pdf', 'page': 71}),\n",
       " Document(page_content='Figure 2: Shooting targets used to represent bias and variance\\n \\nShooting targets, as seen in Figure 2, are not a visual chart used in machine\\nlearning, but it does help to explain bias and variance. Imagine that the center\\nof the target, or the bull’s-eye, perfectly predicts the correct value of your\\nmodel. The dots marked on the target then represent an individual realization\\nof your model based on your training data. In certain cases, the dots will be\\ndensely positioned close to the bull’s-eye, ensuring that predictions made by\\nthe model are close to the actual data. In other cases, the training data will be\\nscattered across the target. The more the dots deviate from the bull’s-eye, the\\nhigher the bias and the less accurate the model will be in its overall predictive\\nability.\\nIn the first target, we can see an example of low bias and low variance. Bias\\nis low because the hits are closely aligned to the center and there is low\\nvariance because the hits are densely positioned in one location.\\nThe second target (located on the right of the first row) shows a case of low\\nbias and high variance. Although the hits are not as close to the bull’s-eye as\\nthe previous example, they are still near to the center and bias is therefore\\nrelatively low. However, there is high variance this time because the hits are', metadata={'source': 'ml.pdf', 'page': 72}),\n",
       " Document(page_content='spread out from each other.\\nThe third target (located on the left of the second row) represents high bias\\nand low variance and the fourth target (located on the right of the second\\nrow) shows high bias and high variance.\\nIdeally, you want a situation where there is low variance and low bias. In\\nreality, though, there is more often a trade-off between optimal bias and\\nvariance. Bias and variance both contribute to error, but it is the prediction\\nerror that you want to minimize, not bias or variance specifically.\\n \\nFigure 3: Model complexity based on prediction error\\n \\nIn Figure 3, we can see two lines moving from left to right. The line above\\nrepresents the test data and the line below represents the training data. From\\nthe left, both lines begin at a point of high prediction error due to low\\nvariance and high bias. As they move from left to right they change to the\\nopposite: high variance and low bias. This leads to low prediction error in the\\ncase of the training data and high prediction error for the test data. In the\\nmiddle of the chart is an optimal balance of prediction error between the\\ntraining and test data. This is a common case of bias-variance trade-off.', metadata={'source': 'ml.pdf', 'page': 73}),\n",
       " Document(page_content='Figure 4: Underfitting on the left and overfitting on the right\\n \\nMismanaging the bias-variance trade-off can lead to poor results. As seen in\\nFigure 4, this can result in the model becoming overly simple and inflexible\\n(underfitting) or overly complex and flexible (overfitting).\\nUnderfitting (low variance, high bias) on the left and overfitting (high\\nvariance, low bias) on the right are shown in these two scatterplots. A natural\\ntemptation is to add complexity to the model (as shown on the right) in order\\nto improve accuracy, but which can, in turn, lead to overfitting. An overfitted\\nmodel will yield accurate predictions from the training data but prove less\\naccurate at formulating predictions from the test data. Overfitting can also\\noccur if the training and test data aren’t randomized before they are split and\\npatterns in the data aren’t distributed across the two segments of data.\\nUnderfitting is when your model is overly simple, and again, has not\\nscratched the surface of the underlying patterns in the dataset. Underfitting\\ncan lead to inaccurate predictions for both the training data and test data.\\nCommon causes of underfitting include insufficient training data to\\nadequately cover all possible combinations, and situations where the training\\nand test data were not properly randomized.\\nTo eradicate both underfitting and overfitting, you may need to modify the\\nmodel’s hyperparameters to ensure that they fit patterns in both the training\\nand test data and not just one-half of the data. A suitable fit should\\nacknowledge major trends in the data and play down or even omit minor\\nvariations. This may also mean re-randomizing the training and test data or\\nadding new data points so as to better detect underlying patterns. However, in\\nmost instances, you will probably need to consider switching algorithms or\\nmodifying your hyperparameters based on trial and error to minimize and', metadata={'source': 'ml.pdf', 'page': 74}),\n",
       " Document(page_content='manage the issue of bias-variance trade-off.\\nSpecifically, this might entail switching from linear regression to non-linear\\nregression to reduce bias by increasing variance. Or it could mean increasing\\n“\\nk\\n” in \\nk\\n-NN to reduce variance (by averaging together more neighbors). A\\nthird example could be reducing variance by switching from a single decision\\ntree (which is prone to overfitting) to a random forest with many decision\\ntrees.\\nAnother effective strategy to combat overfitting and underfitting is to\\nintroduce \\nregularization\\n. Regularization artificially amplifies bias error by\\npenalizing an increase in a model’s complexity. In effect, this add-on\\nparameter provides a warning alert to keep high variance in check while the\\noriginal parameters are being optimized.\\nAnother effective technique to contain overfitting and underfitting in your\\nmodel is to perform cross validation, as covered earlier in Chapter 6, to\\nminimize any discrepancies between the training data and the test data.', metadata={'source': 'ml.pdf', 'page': 75}),\n",
       " Document(page_content='10', metadata={'source': 'ml.pdf', 'page': 76}),\n",
       " Document(page_content='ARTIFICIAL NEURAL NETWORKS\\nThis penultimate chapter on machine learning algorithms brings us to\\nartificial neural networks (ANN) and the gateway to reinforcement learning.\\nArtificial neural networks, also known as neural networks, is a popular\\nmachine learning technique to process data through layers of analysis. The\\nnaming of artificial neural networks was inspired by the algorithm’s\\nresemblance to the human brain.\\n \\nFigure 1: Anatomy of a human neuron\\n \\nThe human brain contains interconnected neurons with dendrites that receive\\ninputs. From these inputs, the neuron produces an electric signal output from\\nthe axon and then emits these signals through axon terminals to other\\nneurons.\\nSimilar to neurons in the human brain, artificial neural networks are formed\\nby interconnected neurons, also called \\nnodes,\\n which interact with each other\\nthrough axons, called \\nedges\\n. In a neural network, the nodes are stacked up in\\nlayers and generally start with a broad base. The first layer consists of raw\\ndata such as numeric values, text, images or sound, which are divided into\\nnodes. Each node then sends information to the next layer of nodes through\\nthe network’s edges.', metadata={'source': 'ml.pdf', 'page': 77}),\n",
       " Document(page_content='Figure 2: The nodes, edges/weights, and sum/activation function of a basic neural network\\n \\nEach edge has a numeric weight (algorithm) that can be altered and\\nformulated based on experience. If the sum of the connected edges satisfies a\\nset threshold, known as the activation function, it will activate a neuron at the\\nnext layer. However, if the sum of the connected edges does not meet the set\\nthreshold, the activation will not be triggered. This results in an \\nall or nothing\\narrangement.\\nNote, also, that the weights along each edge are unique to ensure that the\\nnodes fire differently (as seen in Figure 3) and they don’t all return the same\\noutcome.', metadata={'source': 'ml.pdf', 'page': 78}),\n",
       " Document(page_content=\"Figure 3: Unique edges to produce different outcomes\\n \\nTo train the network through supervised learning, the model’s predicted\\noutput is compared to the actual output (that is known to be correct) and the\\ndifference between these two results is measured and is known as the\\n cost\\n or\\ncost value\\n. The purpose of training is to reduce the cost value until the\\nmodel’s prediction closely matches the correct output. This is achieved by\\nincrementally tweaking the network’s weights until the lowest possible cost\\nvalue is obtained. This process of training the neural network is called \\nback-\\npropagation\\n. Rather than navigate left to right like how data is fed into a\\nneural network, back-propagation is done in reverse and runs from the output\\nlayer from the right towards the input layer on the left.\\nOne of the downsides of neural networks is that they operate as a black-\\nbox in the sense that while the network can approximate accurate outcomes,\\ntracing its structure reveals limited or no insight on the variables that impact\\nthe outcome. For example, when using a neural network to predict the\\nprobable outcome of a Kickstarter (the world's largest funding platform for\\ncreative projects) campaign, the network will analyze a number of variables\\nsuch as campaign category, currency, deadline, and minimum pledge amount,\\nbut it won’t be able to specify their relationships to the final outcome.\\nMoreover, it’s possible for two neural networks with a different topology and\\ndifferent weights to produce the same output, which makes it even more\\ndifficult to trace variable relationships to the output. Examples of non-black-\\nbox models are regression techniques and decision trees.\", metadata={'source': 'ml.pdf', 'page': 79}),\n",
       " Document(page_content='So, when should you use a back-box neural network? In general, neural\\nnetworks are best for solving problems with highly complex patterns and\\nespecially those that are difficult for computers to solve but simple and\\nalmost trivial for humans. An obvious example is a CAPTCHA (Completely\\nAutomated Public Turing test to tell Computers and Humans Apart)\\nchallenge-response test that is used on websites to determine whether an\\nonline user is an actual human. There are numerous blog posts online that\\ndemonstrate how you can crack a CAPTCHA test using neural networks.\\nAnother example is identifying whether a pedestrian will step in the path of\\nan oncoming vehicle as used in self-driving vehicles to avoid the case of an\\naccident.\\n \\nFigure 4: The three general layers of a neural network\\n \\nA typical neural network can be divided into input, hidden, and output layers.\\nData is first received by the input layer, where broad features are detected.\\nThe hidden layer(s) then analyze and process the data. Based on previous\\ncomputations, the data becomes streamlined through the passing of each\\nhidden layer. The final result is shown as the output layer.\\nThe middle layers are considered hidden layers because, like human vision,\\nthey covertly break down objects between the input and output layers. For\\nexample, when humans see four lines connected in the shape of a square we', metadata={'source': 'ml.pdf', 'page': 80}),\n",
       " Document(page_content='instantly recognize those four lines as a square. We don’t notice the lines as\\nfour independent lines with no relationship to each other. Our brain is\\nconscious only of the output layer. Neural networks work much the same way\\nin that they break down data into layers and examine the hidden layers to\\nproduce a final output.\\nWhile there are many techniques to assemble the nodes of a neural network,\\nthe simplest method is the feed-forward network. In a feed-forward network,\\nsignals flow only in one direction and there is no loop in the network.\\nThe most basic form of a feed-forward neural network is the \\nperceptron\\n.\\n \\nFigure 5: Visual representation of a perceptron neural network\\n \\nA perceptron consists of one or more inputs, a processor, and a single output.\\nWithin a perceptron model, inputs:\\n1)\\n     \\nAre fed into the processor (neuron)\\n2)\\n     \\nAre processed\\n3)\\n     \\nGenerate output\\nAs an example, let’s say we have a perceptron consisting of two inputs:\\nInput 1:\\n 3x = 24\\nInput 2: \\n2x = 16\\nWe then add a random weight to these two inputs and they are sent into the\\nneuron to be processed.\\n \\nFigure 6: Weights are added to the perceptron', metadata={'source': 'ml.pdf', 'page': 81}),\n",
       " Document(page_content='Weights\\nInput 1:\\n 0.5\\nInput 2: \\n-1.0\\nNext, multiply each weight by its input:\\nInput 1: \\n24 * 0.5 = 12\\nInput 2: \\n16 * -1.0 = -16\\nPassing the sum of the edge weights through the activation function generates\\nthe perceptron’s output.\\nA key feature of the perceptron is that it only registers two possible\\noutcomes, “1” and “0.” The value of “1” triggers the activation function and\\nthe value of “0” does not. Although the perceptron is binary in nature (1 or\\n0), there are various ways in which we can configure the activation function.\\nIn this example, we made the activation function \\n≥\\n0. This means that if the\\nsum is a positive number or zero, the output is 1. If the sum is a negative\\nnumber, the output is 0.\\n \\nFigure 7: Activation function where the output (y) is 0 when x is negative, and the output (y) is 1 when x is positive\\n \\nThus:\\nInput 1: \\n24 * 0.5 = 12\\nInput 2: \\n16 * -1.0 = -16\\nSum (Σ):\\n 12 + -16 = - 4\\nAs a numeric value less than zero, our result will register as “0” and therefore\\nnot trigger the activation function of the perceptron.\\nHowever, we can also modify the activation threshold to a completely', metadata={'source': 'ml.pdf', 'page': 82}),\n",
       " Document(page_content='different rule, such as:\\nx >\\n \\n3, y = 1\\nx ≤ 3, y = 0\\nFigure 8: Activation function where the output (y) is 0 when x is equal or less than 3, and the output (y) is 1 when x is greater than 3\\n \\nWhen working with a larger model of neural network layers, a value of “1”\\nwill be configured to pass the output to the next layer. Conversely, a “0”\\nvalue is configured to be ignored and will not be passed to the next layer for\\nprocessing.\\nIn supervised learning, perceptrons can be used to train data and develop a\\nprediction model. The steps to training data are as follows:\\n1)\\n     \\nInputs are fed into the processor (neurons/nodes).\\n2)\\n     \\nThe perceptron estimates the value of those inputs.\\n3)\\n     \\nThe perceptron computes the error between the estimate and the\\nactual value.\\n4)\\n     \\nThe perceptron adjusts its weights according to the error.\\n5)\\n     \\nRepeat the previous four steps until you are satisfied with the\\nmodel’s accuracy. The training model can then be applied to the test\\ndata.\\nThe weakness of a perceptron is that, because the output is binary (1 or 0),\\nsmall changes in the weights or bias in any single perceptron within a larger\\nneural network can induce polarizing results. This can lead to dramatic\\nchanges within the network and a complete flip in regards to the final output.\\nAs a result, this makes it very difficult to train an accurate model that can be\\nsuccessfully applied to test data and future data inputs.\\nAn alternative to the perceptron is the \\nsigmoid neuron\\n. A sigmoid neuron is', metadata={'source': 'ml.pdf', 'page': 83}),\n",
       " Document(page_content='very similar to a perceptron, but the presence of a sigmoid function rather\\nthan a binary model now accepts any value between 0 and 1. This enables\\nmore flexibility to absorb small changes in edge weights without triggering\\ninverse results—as the output is no longer binary. In other words, the output\\nresult won’t flip just because of one minor change to an edge weight or input\\nvalue.\\nFigure 9: The sigmoid equation, as first seen in logistic regression\\n \\nWhile more flexible than a perceptron, a sigmoid neuron cannot generate\\nnegative values. Hence, a third option is the \\nhyperbolic tangent function\\n.\\nFigure 10: A hyperbolic tangent function graph\\n \\nWe have so far discussed basic neural networks; to create a more advanced\\nneural network, we can link sigmoid neurons and other classifiers to create a\\nnetwork with a higher number of layers or combine multiple perceptrons to\\nform a multi-layer perceptron.\\nFor analyzing simple patterns, a basic neural network or an alternative\\nclassification tool such as logistic regression and \\nk\\n-nearest neighbors is', metadata={'source': 'ml.pdf', 'page': 84}),\n",
       " Document(page_content='generally sufficient for the purpose of analysis. However, as the patterns in\\nthe data become more complicated—especially in the form of a high number\\nof inputs such as the total number of pixels in an image—a basic or shallow\\nmodel is no longer reliable or capable of analysis. This is because the model\\nbecomes exponentially complex as the number of inputs rises and in the case\\nof neural networks this means more layers to manage more input nodes. A\\nneural network, with a deep number of layers, however, is able to break down\\ncomplex patterns into simpler patterns as demonstrated in Figure 11.\\n \\nFigure 11: Facial recognition using deep learning. \\nSource: kdnuggets.com\\n \\nThis deep network uses edges to detect different physical features to\\nrecognize faces, such as a diagonal line. Like building blocks, the network\\ncombines the node results to classify the input as, say, a human’s face or a\\ncat’s face and then processes that further to recognize a specific individual’s\\nface.\\nThis is known as deep learning. What makes deep learning “deep” is the\\nstacking of at least 5-10 node layers, with advanced object recognition using\\nupwards of 150 layers.\\nObject recognition, as used by self-driving vehicles to recognize objects such\\nas pedestrians and other vehicles, is a popular application of deep learning\\ntoday. Other common applications of deep learning include time series', metadata={'source': 'ml.pdf', 'page': 85}),\n",
       " Document(page_content='analysis to analyze data trends measured over particular time periods or\\nintervals, speech recognition, and text processing tasks including sentiment\\nanalysis, topic segmentation, and named entity recognition. More usage\\nscenarios and commonly paired deep learning techniques are listed in Figure\\n12.\\n \\nFigure 12: Common usage scenarios and paired deep learning techniques\\n \\nAs can be seen from the table, multi-layer perceptrons have been largely\\nsuperseded by new deep learning techniques such as convolution networks,\\nrecurrent networks, deep belief networks, and recursive neural tensor\\nnetworks (RNTN). These more advanced iterations of a neural network can\\nbe used effectively across a number of practical applications that are\\ncurrently in vogue today. Although convolution networks are arguably the\\nmost popular and powerful of deep learning techniques, new methods and\\nvariations are continuously evolving.', metadata={'source': 'ml.pdf', 'page': 86}),\n",
       " Document(page_content='11', metadata={'source': 'ml.pdf', 'page': 87}),\n",
       " Document(page_content=\"DECISION TREES\\nThe fact that neural networks can be applied to a broader range of machine\\nlearning problems than any other technique has led some pundits to hail\\nneural networks as the ultimate machine learning algorithm. However, this is\\nnot to say that neural networks fit the bill as a statistical silver bullet. In\\nvarious cases, neural networks fall short and decision trees are held up as a\\npopular counterargument.\\nThe massive reserve of data and computational resources that neural\\nnetworks demand is one obvious pitfall. Only after training on millions of\\ntagged examples can Google's image recognition engine reliably recognize\\nclasses of simple objects (such as dogs). But how many dog pictures do you\\nneed to show to the average four-year-old before they “get it?”\\nDecision trees, on the other hand, provide high-level efficiency and easy\\ninterpretation. These two benefits make this simple algorithm popular in the\\nspace of machine learning.\\nAs a supervised learning technique, decision trees are used primarily for\\nsolving classification problems, but they can be applied to solve regression\\nproblems too.\\n \\nFigure 1: Example of a regression tree.\\n Source: http://freakonometrics.hypotheses.org/\", metadata={'source': 'ml.pdf', 'page': 88}),\n",
       " Document(page_content='Figure 2: Example of a classification tree. \\nSource: http://blog.akanoo.com\\n \\nClassification trees can use quantitative and categorical data to model\\ncategorical outcomes. Regression trees also use quantitative and categorical\\ndata but instead model quantitative outcomes.\\nDecision trees start with a root node, which acts as a starting point (at the\\ntop), and is followed by splits that produce branches. The\\nstatistical/mathematical term for these branches is \\nedges\\n. The branches then\\nlink to leaves, known also as nodes, which form decision points. A final\\ncategorization is produced when a leaf does not generate any new branches\\nand results in what is known as a terminal node.\\nDecision trees thus not only break down and explain how classification or\\nregression is formulated, but they also produce a neat visual flowchart you\\ncan show to others. The ease of interpretation is a strong advantage of using\\ndecision trees, and they can be applied to a wide range of use cases.\\nReal-life examples include picking a scholarship recipient, assessing an\\napplicant for a home loan, predicting e-commerce sales, or selecting the right\\njob applicant. When a customer or applicant queries why they weren’t\\nselected for a particular scholarship, home loan, job, etc., you can pass them\\nthe decision tree and let them see the decision-making process for\\nthemselves.\\n \\nBuilding a Decision Tree', metadata={'source': 'ml.pdf', 'page': 89}),\n",
       " Document(page_content='Decision trees are built by first splitting data into two groups. This binary\\nsplitting process is then repeated at each branch (layer). The aim is to select a\\nbinary question that best splits the data into two homogenous groups at each\\nbranch of the tree, such that it minimizes the level of data entropy at the next.\\nEntropy is a mathematical term that explains the measure of variance in the\\ndata among different classes. In simple terms, we want the data at each layer\\nto be more homogenous than at the last.\\nWe thus want to pick a “greedy” algorithm that can reduce the level of\\nentropy at each layer of the tree. One such greedy algorithm is the Iterative\\nDichotomizer (ID3), invented by J.R. Quinlan. This is one of three decision\\ntree implementations developed by Quinlan, hence the “3.”\\nID3 applies entropy to determine which binary question to ask at each layer\\nof the decision tree. At each layer, ID3 identifies a variable (converted into a\\nbinary question) that will produce the least entropy at the next layer. Let’s\\nconsider the following example to better understand how this works.\\n \\n \\nVariable 1 (exceeded Key Performance Indicators) produces:\\n- Six promoted employees who exceeded their KPIs (Yes)\\n- Four employees who didn’t exceed their KPIs and who were not promoted\\n(No)\\nThis variable produces two homogenous groups at the next layer of the\\ndecision tree.', metadata={'source': 'ml.pdf', 'page': 90}),\n",
       " Document(page_content='Black = Promoted, White = Not Promoted\\n \\nVariable 2 (leadership capability) produces:\\n-\\n         \\nTwo promoted employees with leadership capabilities (Yes)\\n-\\n         \\nFour promoted employees with no leadership capabilities (No)\\n-\\n         \\nTwo employees with leadership capabilities who were not promoted\\n(Yes)\\n-\\n         \\nTwo employees with no leadership capabilities who were not\\npromoted (No)\\nThis variable produces two groups of mixed data points.\\n \\nBlack = Promoted, White = Not Promoted', metadata={'source': 'ml.pdf', 'page': 91}),\n",
       " Document(page_content='Variable 3 (aged under thirty) produces:\\n-\\n         \\nThree promoted employees aged under thirty (Yes)\\n-\\n         \\nThree promoted employees aged over thirty (No)\\n-\\n         \\nFour employees aged under thirty who were not promoted (Yes)\\nThis variable produces one homogenous group and one mixed group of data\\npoints.\\n \\nBlack = Promoted, White = Not Promoted\\n \\nOf these three variables, variable 1 (Exceeded KPIs) produces the best result\\nwith two perfectly homogenous groups. Variable 3 produces the second best\\nresult, as one leaf is homogenous. Variable 2 produces two leaves that are not\\nhomogenous. Variable 1 would therefore be selected as the first binary\\nquestion to split this dataset.\\nWhether it is ID3 or another algorithm, this process of splitting data into\\nbinary partitions, known as \\nrecursive partitioning\\n, is repeated until a stopping\\ncriterion is met. This stopping point could be based on a range of criteria,\\nsuch as:\\n-\\n         \\nWhen all leaves contain less than 3-5 items\\n-\\n         \\nWhen a branch produces a result that places all items in one binary\\nleaf', metadata={'source': 'ml.pdf', 'page': 92}),\n",
       " Document(page_content='Figure 3: Example of a stopping criteria\\n \\nA caveat to remember when using decision trees is their susceptibility to\\noverfitting. The cause of overfitting, in this case, is the training data. Taking\\ninto account the patterns that exist in your training data, a decision tree is\\nprecise at training the first round of data. However, the same decision tree\\nmay then fail to predict the test data, as there could be rules that it is yet to\\nencounter or because the training or test data were not representative of the\\nentire dataset. Moreover, because decision trees are formed from repeatedly\\nsplitting data points into two partitions, a slight change in how the data is\\nsplit at the top or middle of the tree can dramatically alter the final prediction.\\nThis can produce a different tree altogether! The offender, in this case, is our\\ngreedy algorithm.\\nFrom the very first split of the data, the greedy algorithm fixes its attention on\\npicking a binary question that best partitions data into two homogenous\\ngroups. Like a boy sitting in front of a box of cupcakes, the greedy algorithm\\nis oblivious to the future repercussions of its short-term actions. The binary\\nquestion it uses to initially split the data does not guarantee the most accurate\\nfinal prediction. Rather, a less effective initial split may produce a more\\naccurate outcome.\\nIn sum, decision trees are highly visual and effective at classifying a single\\nset of data, but they can be inflexible and vulnerable to overfitting.\\n \\nRandom Forests\\nRather than striving for the most efficient split at each round of recursive\\npartitioning, an alternative technique is to construct multiple trees and', metadata={'source': 'ml.pdf', 'page': 93}),\n",
       " Document(page_content='combine their predictions to select an optimal path of classification or\\nprediction. This involves a randomized selection of binary questions to grow\\nmultiple different decision trees, known as \\nrandom forests\\n. In the industry,\\nyou will also often hear people refer to this process as “bootstrap\\naggregating” or “bagging.”\\n \\nFigure 4: “Bagging” is a creative abbreviation of “Bootstrap Aggregating”\\n \\nThe key to understanding random forests is to first understand bootstrap\\nsampling. There’s little use in compiling five or ten identical models—there\\nneeds to be some element of variation. This is why bootstrap sampling draws\\non the same dataset but extracts a different variation of the data at each turn.\\nHence, in growing random forests, multiple varying copies of the training\\ndata are first run through each of the trees. For classification problems,\\nbagging undergoes a process of voting to generate the \\nfinal class\\n. The results\\nfrom each tree are compared and voted on to create an optimal tree to\\nproduce the final model, known as the final class. For regression problems,\\nvalue averaging is used to generate a final prediction.\\nBootstrapping is also sometimes called weakly-supervised (you will recall we\\nexplored supervised and unsupervised learning in Chapter 3) because it trains\\nclassifiers using a random subset of features and fewer variables than those\\nactually available.\\n \\nBoosting\\nAnother variant of multiple decision trees is the popular technique of\\nboosting, \\nwhich are a family of algorithms that convert “weak learners” to\\n“strong learners.” The underlying principle of boosting is to add weights to\\niterations that were misclassified in earlier rounds. This can be interpreted as\\nsimilar to a language teacher offering after-school tutoring to the weakest\\nstudents in the class in order to improve the average test results of the entire\\nclass.\\nA popular boosting algorithm is gradient boosting. Rather than selecting\\ncombinations of binary questions at random (like random forests), gradient\\nboosting selects binary questions that improve prediction accuracy for each\\nnew tree. Decision trees are therefore grown sequentially, as each tree is\\ncreated using information derived from the previous decision tree.', metadata={'source': 'ml.pdf', 'page': 94}),\n",
       " Document(page_content='The way this works is that mistakes incurred with the training data are\\nrecorded and then applied to the next round of training data. At each iteration,\\nweights are added to the training data based on the results of the previous\\niteration. Higher weighting is applied to instances that were incorrectly\\npredicted from the training data, and instances that were correctly predicted\\nreceive less weighting. The training and test data are then compared and\\nerrors are again logged in order to inform weighting at each subsequent\\nround. Earlier iterations that do not perform well, and that perhaps\\nmisclassified data, can thus be improved upon through further iterations. This\\nprocess is repeated until there is a low level of error. The final result is then\\nobtained from a weighted average of the total predictions derived from each\\nmodel.\\nWhile this approach mitigates the issue of overfitting, it does so with fewer\\ntrees than the bagging approach. In general, the more trees you add to a\\nrandom forest, the greater its ability to thwart overfitting. Conversely, with\\ngradient boosting, too many trees may cause overfitting and caution should\\nbe taken as new trees are added.\\nOne drawback of using random forests and gradient boosting is that we return\\nto a black-box technique and sacrifice the visual simplicity and ease of\\ninterpretation that comes with a single decision tree.', metadata={'source': 'ml.pdf', 'page': 95}),\n",
       " Document(page_content='12', metadata={'source': 'ml.pdf', 'page': 96}),\n",
       " Document(page_content='ENSEMBLE MODELING\\nOne of the most effective machine learning methodologies is \\nensemble\\nmodeling\\n, also known as \\nensembles\\n. Ensemble modeling combines statistical\\ntechniques to create a model that produces a unified prediction. It is through\\ncombining estimates and following the wisdom of the crowd that ensemble\\nmodeling performs a final classification or outcome with better predictive\\nperformance. Naturally, ensemble models are a popular choice when it comes\\nto machine learning competitions like the Netflix Competition and Kaggle\\ncompetitions.\\nEnsemble models can be classified into various categories including\\nsequential, parallel, homogenous, and heterogeneous. Let’s start by first\\nlooking at sequential and parallel models. For sequential ensemble models,\\nprediction error is reduced by adding weights to classifiers that previously\\nmisclassified data. Gradient boosting and AdaBoost are two examples of\\nsequential models. Conversely, parallel ensemble models work concurrently\\nand reduce error by averaging. Decision trees are an example of this\\ntechnique.\\nEnsemble models can also be generated using a single technique with\\nnumerous variations (known as a homogeneous ensemble) or through\\ndifferent techniques (known as a heterogeneous ensemble). An example of a\\nhomogeneous ensemble model would be numerous decision trees working\\ntogether to form a single prediction (bagging). Meanwhile, an example of a\\nheterogeneous ensemble would be the usage of \\nk\\n-means clustering or a neural\\nnetwork in collaboration with a decision tree model.\\nNaturally, it is important to select techniques that complement each other.\\nNeural networks, for instance, require complete data for analysis, whereas\\ndecision trees can effectively handle missing values. Together, these two\\ntechniques provide added value over a homogeneous model. The neural\\nnetwork accurately predicts the majority of instances that provide a value and\\nthe decision tree ensures that there are no “null” results that would otherwise\\nbe incurred from missing values in a neural network. The other advantage of\\nensemble modeling is that aggregated estimates are generally more accurate', metadata={'source': 'ml.pdf', 'page': 97}),\n",
       " Document(page_content='than any single estimate.\\nThere are various subcategories of ensemble modeling; we have already\\ntouched on two of these in the previous chapter. Four popular subcategories\\nof ensemble modeling are bagging, boosting, a bucket of models, and\\nstacking.\\nBagging\\n, as we know, is short for “boosted aggregating” and is an example\\nof a homogenous ensemble. This method draws upon randomly drawn\\ndatasets and combines predictions to design a unified model based on a\\nvoting process among the training data. Expressed in another way, bagging is\\na special process of model averaging. Random forest, as we know, is a\\npopular example of bagging.\\nBoosting\\n is a popular alternative technique that addresses error and data\\nmisclassified by the previous iteration to form a final model. Gradient\\nboosting and AdaBoost are both popular examples of boosting.\\nA \\nbucket of models\\n trains numerous different algorithmic models using the\\nsame training data and then picks the one that performed most accurately on\\nthe test data.\\nStacking\\n runs multiple models simultaneously on the data and combines\\nthose results to produce a final model. This technique is currently very\\npopular in machine learning competitions, including the Netflix Prize. (Held\\nbetween 2006 and 2009, Netflix offered a prize for a machine learning model\\nthat could improve their recommender system in order to produce more\\neffective movie recommendations. One of the winning techniques adopted a\\nform of linear stacking that combined predictions from multiple predictive\\nmodels.)\\nAlthough ensemble models typically produce more accurate predictions, one\\ndrawback to this methodology is, in fact, the level of sophistication.\\nEnsembles face the same trade-off between accuracy and simplicity as a\\nsingle decision tree versus a random forest. The transparency and simplicity\\nof a simple technique, such as a decision tree or \\nk\\n-nearest neighbors, is lost\\nand instantly mutated into a statistical black-box. Performance of the model\\nwill win out in most cases, but the transparency of your model is another\\nfactor to consider when determining your preferred methodology.', metadata={'source': 'ml.pdf', 'page': 98}),\n",
       " Document(page_content='13', metadata={'source': 'ml.pdf', 'page': 99}),\n",
       " Document(page_content=\"BUILDING A MODEL IN PYTHON\\nAfter examining the statistical underpinnings of numerous algorithms, it’s\\ntime to turn our attention to building an actual machine learning model.\\nAlthough there are various options in regards to programming languages (as\\noutlined in Chapter 4), for this exercise we will use Python because it is quick\\nto learn and it’s an effective programming language for anyone interested in\\nmanipulating and working with large datasets.\\nIf you don't have any experience in programming or programming with\\nPython, there’s no need to worry. The key purpose of this chapter is to\\nunderstand the methodology and steps behind building a basic machine\\nlearning model.\\nIn this exercise, we will design a house price valuation system using gradient\\nboosting by following these six steps:\\n1)\\n     \\nSet up the development environment\\n2)\\n     \\nImport the dataset\\n3)\\n     \\nScrub the dataset\\n4)\\n     \\nSplit the data into training and test data\\n5)\\n     \\nSelect an algorithm and configure its hyperparameters\\n6)\\n     \\nEvaluate the results\\n \\n1) Set up the development environment\\nThe first step is to prepare our development environment. For this exercise,\\nwe will be working in Jupyter Notebook, which is an open-source web\\napplication that allows editing and sharing of notebooks.\\nYou can download Jupyter Notebook from: http://jupyter.org/install.html\\nJupyter Notebook can be installed using the Anaconda Distribution or\\nPython’s package manager, pip. There are instructions available on the\\nJupyter Notebook website that outline both options. As an experienced\\nPython user, you may wish to install Jupyter Notebook via pip. For\\nbeginners, I recommend selecting the Anaconda Distribution option, which\\noffers an easy click-and-drag setup.\", metadata={'source': 'ml.pdf', 'page': 100}),\n",
       " Document(page_content='This particular installation option will direct you to the Anaconda website.\\nFrom there, you can select your preferred installation for Windows, macOS,\\nor Linux. Again, you can find instructions available on the Anaconda website\\naccording to your choice of operating system.\\nAfter installing Anaconda to your machine, you will have access to a number\\nof data science applications including rstudio, Jupyter Notebook, and\\ngraphviz for data visualization. For this exercise, you will need to select\\nJupyter Notebook by clicking on “Launch” inside the Jupyter Notebook tab.\\n \\nFigure 1: The Anaconda Navigator portal\\n \\nTo initiate Jupyter Notebook, run the following command from the Terminal\\n(for Mac/Linux) or Command Prompt (for Windows):\\n \\njupyter notebook\\n \\nTerminal/Command Prompt will then generate a URL for you to copy and\\npaste into your web browser. Example: http://localhost:8888/\\nCopy and paste the generated URL into your web browser to load Jupyter\\nNotebook. Once you have Jupyter Notebook open in your browser, click on', metadata={'source': 'ml.pdf', 'page': 101}),\n",
       " Document(page_content=\"“New” in the top right-hand corner of the web application to create a new\\n“Notepad” project, and then select “Python 3.”\\nThe final step is to install the necessary libraries required to complete this\\nexercise. You will need to install Pandas and a number of libraries from\\nScikit-learn into the notepad.\\nIn machine learning, each project will vary in regards to the libraries required\\nfor import. For this particular exercise, we are using gradient boosting\\n(ensemble modeling) and mean absolute error to measure performance.\\nYou will need to import each of the following libraries and functions by\\nentering these exact commands in Jupyter Notebook:\\n \\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import ensemble\\nfrom sklearn.metrics import mean_absolute_error\\nfrom sklearn.externals import joblib\\n \\nDon’t worry if you don’t recognize each of the imported libraries in the code\\nsnippet above. These libraries will be referred to in later steps.\\n \\n2) Import the dataset\\nThe next step is to import the dataset. For this exercise, I have selected a free\\nand publicly available dataset from kaggle.com which contains house, unit,\\nand townhouse prices in Melbourne, Australia. This dataset comprises data\\nscraped from publicly available listings posted weekly on\\nwww.domain.com.au. The dataset contains 14,242 property listings and 21\\nvariables including address, suburb, land size, number of rooms, price,\\nlongitude, latitude, postcode, etc.\\nPlease note that the property values in this dataset are expressed in Australian\\nDollars—$1 AUD is approximately $0.77 USD (as of 2017).\\nDownload the Melbourne Housing Market dataset from this link:\\nhttps://www.kaggle.com/anthonypino/melbourne-housing-market\\nAfter registering a free account and logging into kaggle.com, download the\\ndataset as a zip file. Next, unzip the downloaded file and import into Jupyter\\nNotebook. To import the dataset, you can utilize the read_csv function to\\nload the data into a Pandas dataframe.\\n \\ndf = pd.read_csv('~/Downloads/Melbourne_housing_FULL-26-09-2017.csv')\", metadata={'source': 'ml.pdf', 'page': 102}),\n",
       " Document(page_content=\"This command will directly import the dataset. However, please note that the\\nexact file path will depend on the saved location of your dataset. For\\nexample, if you saved the CSV file to your desktop, you would need to read\\nin the .csv file using the following command:\\n \\ndf = pd.read_csv('~/Desktop/Melbourne_housing_FULL-26-09-2017.csv')\\n \\nIn my case, I imported the dataset from my Downloads folder. As you move\\nforward in machine learning and data science, it’s important that you save\\ndatasets and projects in standalone and named folders for organized access. If\\nyou opt to save the .csv into the same folder as your Jupyter Notebook, you\\nwon’t need to append a directory name or “~/.”\\nNext, to preview the dataframe within Jupyter Notebook, enter the following\\ncommand, with “n” representing the number of rows you wish to preview in\\nrelation to the head row.\\n \\ndf.head(n=5)\\n \\nRight-click and select “Run” or navigate from the Jupyter Notebook menu:\\nCell > Run All\\n \\nFigure 2: Previewing a dataframe in Jupyter Notebook\\n \\nThis will populate the dataset within Jupyter Notebook as shown in Figure 2.\\nThis step is not mandatory, but it is a useful technique for reviewing your\\ndataset inside Jupyter Notebook.\\n \\n3) Scrub the dataset\", metadata={'source': 'ml.pdf', 'page': 103}),\n",
       " Document(page_content=\"The next stage is to scrub the dataset. Remember, scrubbing is the process of\\nrefining your dataset. This involves modifying or removing incomplete,\\nirrelevant or duplicated data. It may also entail converting text-based data to\\nnumerical values and the redesigning of features.\\nIt is important to note that the scrubbing process can take place before or after\\nimporting the dataset into Jupyter Notebook. For example, the creator of the\\nMelbourne Housing Market dataset has misspelled “Longitude” and\\n“Latitude” in the head columns. As we will not be examining these two\\nvariables in our exercise, there is no need to make any changes. If, though,\\nwe did wish to include these two variables in our model, it would be prudent\\nto first fix this error.\\nFrom a programming perspective, spelling mistakes in the column titles do\\nnot pose any problems as long as we apply the same keyword spelling to\\nperform our commands. However, this misnaming of columns could lead to\\nhuman errors, especially if you are sharing your code with team members. To\\navoid any potential confusion, it’s best to fix spelling mistakes and other\\nsimple errors in the source file before importing the dataset into Jupyter\\nNotebook or another development environment. You can do this by opening\\nthe CSV file in Microsoft Excel (or equivalent program), editing the dataset,\\nand then resaving it again as a CSV file.\\nWhile simple errors can be corrected within the source file, major structural\\nchanges to the dataset such as feature engineering are best performed in the\\ndevelopment environment for added flexibility and to preserve the dataset for\\nlater use. For instance, in this exercise, we will be implementing feature\\nengineering to remove a number of columns from the dataset, but we may\\nlater change our mind about which columns we wish to include.\\nManipulating the composition of the dataset in the development environment\\nis less permanent and generally much simpler and quicker than doing so\\ndirectly in the source file.\\n \\nScrubbing Process\\nLet’s first remove columns from the dataset that we don’t wish to include in\\nthe model by using the del df[' '] function and entering the vector (column)\\ntitles that we wish to remove.\\n \\n# The misspellings of “longitude” and “latitude” are used, as the two misspellings were not corrected in\\nthe source file.\", metadata={'source': 'ml.pdf', 'page': 104}),\n",
       " Document(page_content=\"del df['Address']\\ndel df['Method']\\ndel df['SellerG']\\ndel df['Date']\\ndel df['Postcode']\\ndel df['Lattitude']\\ndel df['Longtitude']\\ndel df['Regionname']\\ndel df['Propertycount']\\n \\nThe Address, Regionname, and Propertycount columns were removed as\\nproperty location is covered in other columns (Suburb and CouncilArea) and\\nbecause we want to minimize non-numerical information (e.g. Address and\\nRegionname). Postcode, Latitude, and Longitude were also removed because,\\nagain, property location is contained in the Suburb and CouncilArea columns.\\nMy assumption is that Suburb and CouncilArea tend to have more sway in\\nbuyers’ minds than Postcode, Latitude, and Longitude—although Address\\ndeserves an honorable mention.\\nMethod, SellerG, and Date were also removed because they were deemed to\\nhave less relevance in comparison to other variables. This is not to say that\\nthese variables don’t impact property prices, rather the other eleven\\nindependent variables are sufficient for building a basic model. We can\\ndecide to add any of these variables into the model later, and you may choose\\nto include them in your own model.\\nThe remaining eleven independent variables (represented as X) in the dataset\\nare Suburb, Rooms, Type, Distance, Bedroom2, Bathroom, Car, Landsize,\\nBuildingArea, YearBuilt, and CouncilArea. The twelfth variable, located in\\nthe fifth column of the downloaded dataset, is the dependent variable, which\\nis Price (represented as y). As mentioned, decision trees (including gradient\\nboosting and random forests) are adept at managing large and high-\\ndimensional datasets with a high number of variables.\\nThe next step for scrubbing the dataset is to remove any missing values.\\nAlthough there are numerous methods to manage missing values (e.g.\\ncalculating the mean, the median, or deleting missing values altogether), for\\nthis exercise, we want to keep it as simple as possible and we’ll therefore not\\nbe examining rows with missing values. The obvious downside is that we\\nhave less data to analyze. As a beginner, it makes sense to master complete\\ndatasets before adding an extra dimension of difficulty in attempting to deal\\nwith missing values. Unfortunately, in the case of our sample dataset, we \\ndo\", metadata={'source': 'ml.pdf', 'page': 105}),\n",
       " Document(page_content=\"have a lot of missing values! Nonetheless, we still have ample rows available\\nto proceed with building our model.\\nThe following Pandas function can be used to remove rows with missing\\nvalues:\\n \\ndf.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\\n \\nKeep in mind that it’s important to drop rows with missing values after\\napplying the del df function to remove columns (as shown in the previous\\nstep). This way, there’s a better chance that more rows from the original\\ndataset will be preserved. Imagine dropping a whole row because it was\\nmissing the value for a variable that would be later deleted like the post code\\nin our model!\\nNext, let’s convert columns that contain non-numerical data to numerical\\nvalues using one-hot encoding. With Pandas, one-hot encoding can be\\nperformed using the get_dummies function:\\n \\nfeatures_df = pd.get_dummies(df, columns=['Suburb', 'CouncilArea', 'Type'])\\n \\nThis command converts column values for Suburb, CouncilArea, and Type\\ninto numerical values through the application of one-hot encoding.\\nNext, we need to remove the “Price” column because this column will act as\\nour dependent variable (y) and for now we are only examining the eleven\\nindependent variables (X).\\n \\ndel features_df['Price']\\n \\nFinally, create X and y arrays from the dataset using the matrix data type\\n(as_matrix). The X array contains the independent variables and the y array\\ncontains the dependent variable of Price.\\n \\nX = features_df.as_matrix()\\ny = df['Price'].as_matrix()\\n \\n4) Split the dataset\\nWe are now at the stage of splitting the data into training and test segments.\\nFor this exercise, we will proceed with a standard 70/30 split by calling the\", metadata={'source': 'ml.pdf', 'page': 106}),\n",
       " Document(page_content='Scikit-learn function below with an argument of “0.3.” The dataset’s rows are\\nalso shuffled randomly to avoid bias using the random_state function.\\n \\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)', metadata={'source': 'ml.pdf', 'page': 107}),\n",
       " Document(page_content=\"5) Select the algorithm and configure its hyperparameters\\nAs you will recall, we are using the gradient boosting algorithm for this\\nexercise, as shown.\\n \\nmodel = ensemble.GradientBoostingRegressor(\\n    n_estimators=150,\\n    learning_rate=0.1,\\n    max_depth=30,\\n    min_samples_split=4,\\n    min_samples_leaf=6,\\n    max_features=0.6,\\n    loss='huber'\\n)\\n \\nThe first line is the algorithm itself (gradient boosting) and comprises just\\none line of code. The lines below dictate the hyperparameters for this\\nalgorithm.\\nn_estimators\\n represents how many decision trees to build. Remember that a\\nhigh number of trees will generally improve accuracy (up to a certain point),\\nbut it will also increase the model’s processing time. Above, I have selected\\n150 decision trees as an initial starting point.\\nlearning_rate\\n controls the rate at which additional decision trees influence\\nthe overall prediction. This effectively shrinks the contribution of each tree\\nby the set learning_rate. Inserting a low rate here, such as 0.1, should\\nimprove accuracy.\\nmax_depth\\n defines the maximum number of layers (depth) for each decision\\ntree. If “None” is selected, then nodes expand until all leaves are pure or until\\nall leaves contain less than min_samples_leaf. Here, I have selected a high\\nmaximum number of layers (30), which will have a dramatic effect on the\\nfinal result, as we will see later.\\nmin_samples_split \\ndefines the minimum number of samples required to\\nexecute a new binary split. For example, min_samples_split = 10 means there\\nmust be ten available samples in order to create a new branch.\\nmin_samples_leaf\\n represents the minimum number of samples that must\\nappear in each child node (leaf) before a new branch can be implemented.\\nThis helps to mitigate the impact of outliers and anomalies in the form of a\\nlow number of samples found in one leaf as a result of a binary split. For\\nexample, min_samples_leaf =\\n \\n4 requires there to be at least four available\", metadata={'source': 'ml.pdf', 'page': 108}),\n",
       " Document(page_content='samples within each leaf for a new branch to be created.\\nmax_features \\nis the total number of features presented to the model when\\ndetermining the best split. As mentioned in Chapter 11, random forests and\\ngradient boosting restrict the total number of features shown to each\\nindividual tree to create multiple results that can be voted upon later.\\nIf the max_features value is an integer (whole number), the model will\\nconsider max_features at each split (branch). If the value is a float (e.g. 0.6),\\nthen max_features is the percentage of total features randomly selected.\\nAlthough max_features sets a maximum number of features to consider in\\nidentifying the best split, total features may exceed the max_features limit if\\nno split can initially be made.\\nloss \\ncalculates the model\\'s error rate. For this exercise, we are using huber\\nwhich protects against outliers and anomalies. Alternative error rate options\\ninclude ls (least squares regression), lad (least absolute deviations), and\\nquantile (quantile regression). Huber is actually a combination of ls and lad.\\nTo learn more about gradient boosting hyperparameters, you may refer to the\\nScikit-learn website:\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\\nAfter imputing the model’s hyperparameters, we will implement Scikit-\\nlearn\\'s fit function to start the model training process.\\nmodel.fit(X_train, y_train)\\n \\nLastly, we need to use Scikit-learn to save the training model as a file using\\nthe joblib.dump function, which was imported into Jupyter Notebook in Step\\n1. This will allow us to use the training model again in the future for\\npredicting new real estate property values, without needing to rebuild the\\nmodel from scratch.\\n \\njoblib.dump(model, \\'house_trained_model.pkl\\')\\n \\n6) Evaluate the results\\nAs mentioned earlier, for this exercise we will use mean absolute error to\\nevaluate the accuracy of the model.\\n \\nmse = mean_absolute_error(y_train, model.predict(X_train))\\nprint (\"Training Set Mean Absolute Error: %.2f\" % mse)\\n \\nHere, we input our y values, which represent the correct results from the', metadata={'source': 'ml.pdf', 'page': 109}),\n",
       " Document(page_content='training dataset. The model.predict function is then called on the X training\\nset\\n \\nand will generate a prediction with up to two decimal places. The mean\\nabsolute error function will then compare the difference between the model’s\\nexpected predictions and the actual values. The same process is repeated with\\nthe test data.\\n \\nmse = mean_absolute_error(y_test, model.predict(X_test))\\nprint (\"Test Set Mean Absolute Error: %.2f\" % mse)\\n \\nLet’s now run the entire model by right-clicking and selecting “Run” or\\nnavigating from the Jupyter Notebook menu: Cell > Run All.\\nWait a few seconds for the computer to process the training model. The\\nresults, as shown below, will then appear at the bottom of the notepad.\\n \\nTraining Set Mean Absolute Error: 27157.02\\nTest Set Mean Absolute Error: 169962.99\\n \\nFor this exercise, our training set mean absolute error is $27,157.02 and the\\ntest set mean absolute error is $169,962.99. This means that on average, the\\ntraining set miscalculated the actual property value by a mere $27,157.02.\\nHowever, the test set miscalculated by an average of $169,962.99.\\nThis means that our training model was very accurate at predicting the actual\\nvalue of properties contained in the training data. While $27,157.02 may\\nseem like a lot of money, this average error value is low given the maximum\\nrange of our dataset is $8 million. As many of the properties in the dataset are\\nin excess of seven figures ($1,000,000+), $27,157.02 constitutes a reasonably\\nlow error rate.\\nBut how did the model fare with the test data? These results are less accurate.\\nThe test data provided less indicative predictions with an average error rate of\\n$169,962.99. A high discrepancy between the training and test data is usually\\na key indicator of overfitting. As our model is tailored to the training data, it\\nstumbled when predicting the test data, which probably contains new patterns\\nthat the model hasn’t adjusted for. The test data, of course, is likely to contain\\nslightly different patterns and new potential outliers and anomalies.\\nHowever, in this case, the difference between the training and test data is\\nexacerbated by the fact that we configured the model to overfit the training\\ndata. An example of this issue was setting max_depth to “30.” Although\\nsetting a high max_depth improves the chances of the model finding patterns', metadata={'source': 'ml.pdf', 'page': 110}),\n",
       " Document(page_content='in the training data, it does tend to lead to overfitting. Another possible cause\\nis a poor split of the training and test data, but for this model the data was\\nrandomized using Scikit-learn.\\nLastly, please take into account that because the training and test data are\\nshuffled randomly, your own results will differ slightly when replicating this\\nmodel on your own machine.', metadata={'source': 'ml.pdf', 'page': 111}),\n",
       " Document(page_content='14', metadata={'source': 'ml.pdf', 'page': 112}),\n",
       " Document(page_content='MODEL OPTIMIZATION\\nIn the previous chapter we built our first supervised learning model. We now\\nwant to improve its accuracy and reduce the effects of overfitting. A good\\nplace to start is modifying the model’s hyperparameters.\\nWithout changing any other hyperparameters, let’s first start by modifying\\nmax_depth from “30” to “5.” The model now generates the following results:\\n \\n# Results will differ due to the randomized data split\\nTraining Set Mean Absolute Error: 129412.51\\n \\nAlthough the mean absolute error of the training set is higher, this helps\\nreduce the problem of overfitting and should improve the results of the test\\ndata. Another step to optimize the model is to add more trees. If we set\\nn_estimators to 250, we see this result:\\n \\n# Results will differ as per the randomized data split\\nTraining Set Mean Absolute Error: 118130.46\\nTest Set Mean Absolute Error: 159886.32\\n \\nThis second optimization reduces the training set’s absolute error rate by\\napproximately $11,000 and we now have a smaller gap between our training\\nand test results for mean absolute error.\\nTogether, these two optimizations underline the importance of maximizing\\nand understanding the impact of individual hyperparameters. If you decide to\\nreplicate this supervised machine learning model at home, I recommend that\\nyou test modifying each of the hyperparameters individually and analyze\\ntheir impact on mean absolute error. In addition, you will notice changes in\\nthe machine’s processing time based on the hyperparameters selected. For\\ninstance, setting max_depth to “5” reduces total processing time compared to\\nwhen it was set to “30” because the maximum number of branch layers are\\nsignificantly less. Processing speed and resources will become an important\\nconsideration as you move on to working with larger datasets.\\nAnother important optimization technique is feature selection. As you will', metadata={'source': 'ml.pdf', 'page': 113}),\n",
       " Document(page_content='recall, we removed nine features while scrubbing our dataset. Now might be\\na good time to reconsider those features and analyze whether they have an\\neffect on the overall accuracy of the model. “SellerG” would be an interesting\\nfeature to add to the model because the real estate company selling the\\nproperty could have some impact on the final selling price.\\nAlternatively, dropping features from the current model may reduce\\nprocessing time without having a significant effect on accuracy—or may\\neven improve accuracy. To select features effectively, it is best to isolate\\nfeature modifications and analyze the results, rather than applying various\\nchanges at once.\\nWhile manual trial and error can be an effective technique to understand the\\nimpact of variable selection and hyperparameters, there are also automated\\ntechniques for model optimization, such as \\ngrid search\\n.\\n \\nGrid search allows\\nyou to list a range of configurations you wish to test for each hyperparameter,\\nand then methodically tests each of those possible hyperparameters. An\\nautomated voting process takes place to determine the optimal model. As the\\nmodel must test each possible combination of hyperparameters, grid search\\ndoes take a long time to run! Example code for grid search is shown at the\\nend of this chapter.\\nFinally, if you wish to use a different supervised machine learning algorithm\\nand not gradient boosting, much of the code used in this exercise can be\\nreplicated. For instance, the same code can be used to import a new dataset,\\npreview the dataframe, remove features (columns), remove rows, split and\\nshuffle the dataset, and evaluate mean absolute error.\\nhttp://scikit-learn.org is a great resource to learn more about other algorithms\\nas well as the gradient boosting used in this exercise.\\n \\nFor a copy of the code, please contact the author at\\noliver.theobald@scatterplotpress.com or see the code example below. In\\naddition, if you have troubles implementing the model using the code found\\nin this book, please feel free to contact the author by email for extra\\nassistance at no cost.\\n \\nCode for the Optimized Model\\n# Import libraries\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split', metadata={'source': 'ml.pdf', 'page': 114}),\n",
       " Document(page_content=\"from sklearn import ensemble\\nfrom sklearn.metrics import mean_absolute_error\\nfrom sklearn.externals import joblib\\n \\n# Read in data from CSV\\ndf =  pd.read_csv('~/Downloads/Melbourne_housing_FULL-26-09-2017.csv')\\n \\n# Delete unneeded columns\\ndel df['Address']\\ndel df['Method']\\ndel df['SellerG']\\ndel df['Date']\\ndel df['Postcode']\\ndel df['Lattitude']\\ndel df['Longtitude']\\ndel df['Regionname']\\ndel df['Propertycount']\\n \\n# Remove rows with missing values\\ndf.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\\n \\n# Convert non-numerical data using one-hot encoding\\nfeatures_df = pd.get_dummies(df, columns=['Suburb', 'CouncilArea', 'Type'])\\n \\n# Remove price\\ndel features_df['Price']\\n \\n# Create X and y arrays from the dataset\\nX = features_df.as_matrix()\\ny = df['Price'].as_matrix()\\n \\n# Split data into test/train set (70/30 split) and shuffle\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\\n \\n# Set up algorithm\\nmodel = ensemble.GradientBoostingRegressor(\\n    n_estimators=250,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    min_samples_split=4,\\n    min_samples_leaf=6,\\n    max_features=0.6,\\n    loss='huber'\\n)\\n \\n# Run model on training data\\nmodel.fit(X_train, y_train)\", metadata={'source': 'ml.pdf', 'page': 115}),\n",
       " Document(page_content='# Save model to file\\njoblib.dump(model, \\'trained_model.pkl\\')\\n \\n# Check model accuracy (up to two decimal places)\\nmse = mean_absolute_error(y_train, model.predict(X_train))\\nprint (\"Training Set Mean Absolute Error: %.2f\" % mse)\\n \\nmse = mean_absolute_error(y_test, model.predict(X_test))\\nprint (\"Test Set Mean Absolute Error: %.2f\" % mse)\\n \\nCode for Grid Search Model\\n# Import libraries, including GridSearchCV\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import ensemble\\nfrom sklearn.metrics import mean_absolute_error\\nfrom sklearn.externals import joblib\\nfrom sklearn.model_selection import GridSearchCV\\n \\n# Read in data from CSV\\ndf = pd.read_csv(\\'~/Downloads/Melbourne_housing_FULL-26-09-2017.csv\\')\\n \\n# Delete unneeded columns\\ndel df[\\'Address\\']\\ndel df[\\'Method\\']\\ndel df[\\'SellerG\\']\\ndel df[\\'Date\\']\\ndel df[\\'Postcode\\']\\ndel df[\\'Lattitude\\']\\ndel df[\\'Longtitude\\']\\ndel df[\\'Regionname\\']\\ndel df[\\'Propertycount\\']\\n \\n# Remove rows with missing values\\ndf.dropna(axis=0, how=\\'any\\', thresh=None, subset=None, inplace=True)\\n \\n# Convert non-numerical data using one-hot encoding\\nfeatures_df = pd.get_dummies(df, columns=[\\'Suburb\\', \\'CouncilArea\\', \\'Type\\'])\\n \\n# Remove price\\ndel features_df[\\'Price\\']\\n \\n# Create X and y arrays from the dataset\\nX = features_df.as_matrix()\\ny = df[\\'Price\\'].as_matrix()\\n \\n# Split data into test/train set (70/30 split) and shuffle', metadata={'source': 'ml.pdf', 'page': 116}),\n",
       " Document(page_content='X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\\n \\n# Input algorithm\\nmodel = ensemble.GradientBoostingRegressor()\\n \\n# Set the configurations that you wish to test\\nparam_grid = {\\n    \\'n_estimators\\': [300, 600, 1000],\\n    \\'max_depth\\': [7, 9, 11],\\n    \\'min_samples_split\\': [3, 4, 5],\\n    \\'min_samples_leaf\\': [5, 6, 7],\\n    \\'learning_rate\\': [0.01, 0.02, 0.6, 0.7],\\n    \\'max_features\\': [0.8, 0.9],\\n    \\'loss\\': [\\'ls\\', \\'lad\\', \\'huber\\']\\n}\\n \\n# Define grid search. Run with four CPUs in parallel if applicable.\\ngs_cv = GridSearchCV(model, param_grid, n_jobs=4)\\n \\n# Run grid search on training data\\ngs_cv.fit(X_train, y_train)\\n \\n# Print optimal hyperparameters\\nprint(gs_cv.best_params_)\\n \\n# Check model accuracy (up to two decimal places)\\nmse = mean_absolute_error(y_train, gs_cv.predict(X_train))\\nprint(\"Training Set Mean Absolute Error: %.2f\" % mse)\\n \\nmse = mean_absolute_error(y_test, gs_cv.predict(X_test))\\nprint(\"Test Set Mean Absolute Error: %.2f\" % mse)', metadata={'source': 'ml.pdf', 'page': 117}),\n",
       " Document(page_content='BUG BOUNTY\\nThank you for reading this absolute beginners’ introduction to machine\\nlearning. While not customary practice in the publishing industry, we do offer\\na financial reward to readers for locating errors or bugs found in this book.\\nFor this genre of writing—statistical-based data modeling—it is not\\nuncommon for errors to emerge in the eye of the beholder. In other words,\\nit’s natural for readers to occasionally misinterpret diagrams, copy code\\nincorrectly or misread important concepts. This is human nature, but to avoid\\nreaders attacking the author with a negative review and affecting future sales\\nof this title, we invite you to report any bugs by first sending us an email at\\noliver.theobald@scatterplotpress.com\\nThis way we can supply further explanations and examples over email to\\ncalibrate your understanding, or in cases where you’re right and we’re wrong,\\nwe offer a monetary reward of USD $20. This way you can make a tidy profit\\nfrom your feedback and we can update the book to improve the standard of\\ncontent for other readers.', metadata={'source': 'ml.pdf', 'page': 118}),\n",
       " Document(page_content='FURTHER RESOURCES\\nThis section lists relevant learning materials for readers that wish to progress\\nfurther in the field of machine learning. Please note that certain details listed\\nin this section, including prices, may be subject to change in the future.\\n \\n| Machine Learning |\\n \\nMachine Learning\\nFormat: \\nCoursera course\\nPresenter: \\nAndrew Ng\\nCost: \\nFree\\nSuggested Audience:\\n Beginners (especially those with a preference for\\nMATLAB)\\nA free and well-taught introduction from Andrew Ng, one of the most\\ninfluential figures in this field. This course has become a virtual rite of\\npassage for anyone interested in machine learning.\\n \\nProject 3: Reinforcement Learning\\nFormat: \\nOnline blog tutorial\\nAuthor: \\nEECS Berkeley\\nSuggested Audience:\\n Upper intermediate to advanced\\nA practical demonstration of reinforcement learning, and Q-learning\\nspecifically, explained through the game Pac-Man.\\n \\n| Basic Algorithms |\\n \\nMachine Learning With Random Forests And Decision Trees: A Visual\\nGuide For Beginners \\nFormat:\\n E-book\\nAuthor: \\nScott Hartshorn\\nSuggested Audience: \\nEstablished beginners', metadata={'source': 'ml.pdf', 'page': 119}),\n",
       " Document(page_content=\"A short, affordable (USD $3.20), and engaging read on decision trees and\\nrandom forests with detailed visual examples, useful practical tips, and clear\\ninstructions.\\n \\nLinear Regression And Correlation: A Beginner's Guide\\nFormat:\\n E-book\\nAuthor:\\n Scott Hartshorn\\nSuggested Audience: \\nAll\\nA well-explained and affordable (USD $3.20) introduction to linear\\nregression, as well as correlation.\\n \\n| The Future of AI |\\n \\nThe Inevitable: Understanding the 12 Technological Forces That Will\\nShape Our Future\\nFormat:\\n E-Book, Book, Audiobook\\nAuthor: \\nKevin Kelly\\nSuggested Audience: \\nAll (with an interest in the future)\\nA well-researched look into the future with a major focus on AI and machine\\nlearning by The New York Times Best Seller Kevin Kelly. Provides a guide\\nto twelve technological imperatives that will shape the next thirty years.\\n \\nHomo Deus: A Brief History of Tomorrow\\nFormat:\\n E-Book, Book, Audiobook\\nAuthor: \\nYuval Noah Harari\\nSuggested Audience: \\nAll (with an interest in the future)\\nAs a follow-up title to the success of \\nSapiens: A Brief History of Mankind,\\nYuval Noah Harari examines the possibilities of the future with notable\\nsections of the book examining machine consciousness, applications in AI,\\nand the immense power of data and algorithms.\\n \\n| Programming |\\n \\nLearning Python\\n, \\n5th Edition\", metadata={'source': 'ml.pdf', 'page': 120}),\n",
       " Document(page_content='Format:\\n E-Book, Book\\nAuthor: \\nMark Lutz\\nSuggested Audience: \\nAll (with an interest in learning Python)\\nA comprehensive introduction to Python published by O’Reilly Media.\\n \\nHands-On Machine Learning with Scikit-Learn and TensorFlow:\\nConcepts, Tools, and Techniques to Build Intelligent Systems\\nFormat:\\n E-Book, Book\\nAuthor: \\nAurélien Géron\\nSuggested Audience: \\nAll (with an interest in programming in Python, Scikit-\\nLearn and TensorFlow)\\nAs a highly popular O’Reilly Media book written by machine learning\\nconsultant Aurélien Géron, this is an excellent advanced resource for anyone\\nwith a solid foundation of machine learning and computer programming.\\n \\n| Recommendation Systems |\\n \\nThe Netflix Prize and Production Machine Learning Systems: An Insider\\nLook\\nFormat:\\n Blog\\nAuthor: \\nMathworks\\nSuggested Audience: \\nAll\\nA very interesting blog article demonstrating how Netflix applies machine\\nlearning to form movie recommendations.\\n \\nRecommender Systems\\nFormat:\\n Coursera course\\nPresenter: \\nThe University of Minnesota\\nCost: \\nFree 7-day trial or included with $49 USD Coursera subscription\\nSuggested Audience: \\nAll\\nTaught by the University of Minnesota, this Coursera specialization covers\\nfundamental recommender system techniques including content-based and\\ncollaborative filtering as well as non-personalized and project-association\\nrecommender systems.\\n.', metadata={'source': 'ml.pdf', 'page': 121}),\n",
       " Document(page_content='| Deep Learning |\\n \\nDeep Learning Simplified\\nFormat:\\n Blog\\nChannel: \\nDeepLearning.TV\\nSuggested Audience: \\nAll\\nA short video series to get you up to speed with deep learning. Available for\\nfree on YouTube.\\n \\nDeep Learning Specialization: Master Deep Learning, and Break into AI\\nFormat:\\n Coursera course\\nPresenter: \\ndeeplearning.ai and NVIDIA\\nCost: \\nFree 7-day trial or included with $49 USD Coursera subscription\\nSuggested Audience: \\nIntermediate to advanced (with experience in Python)\\nA robust curriculum for those wishing to learn how to build neural networks\\nin Python and TensorFlow, as well as career advice, and how deep learning\\ntheory applies to industry.\\n \\nDeep Learning Nanodegree\\nFormat:\\n Udacity course\\nPresenter: \\nUdacity\\nCost: \\n$599 USD\\nSuggested Audience: \\nUpper beginner to advanced, with basic experience in\\nPython\\nComprehensive and practical introduction to convolutional neural networks,\\nrecurrent neural networks, and deep reinforcement learning taught online\\nover a four-month period. Practical components include building a dog breed\\nclassifier, generating TV scripts, generating faces, and teaching a quadcopter\\nhow to fly.\\n \\n| Future Careers |\\n \\nWill a Robot Take My Job?\\nFormat:\\n Online article', metadata={'source': 'ml.pdf', 'page': 122}),\n",
       " Document(page_content=\"Author: \\nThe BBC\\nSuggested Audience: \\nAll\\nCheck how safe your job is in the AI era leading up to the year 2035.\\n \\nSo You Wanna Be a Data Scientist? A Guide to 2015's Hottest Profession\\nFormat:\\n Blog\\nAuthor: \\nTodd Wasserman\\nSuggested Audience: \\nAll\\nExcellent insight into becoming a data scientist.\\n \\nThe Data Science Venn Diagram\\nFormat:\\n Blog\\nAuthor: \\nDrew Conway\\nSuggested Audience: \\nAll\\nThe popular 2010 data science diagram designed by Drew Conway.\", metadata={'source': 'ml.pdf', 'page': 123}),\n",
       " Document(page_content='DOWNLOADING DATASETS\\nBefore you can start practicing algorithms and building machine learning\\nmodels, you will first need data. For beginners starting out in machine\\nlearning, there are a number of options. One is to source your own dataset\\nfrom writing a web crawler in Python or utilizing a click-and-drag tool such\\nas Import.io to crawl the Internet. However, the easiest and best option to get\\nstarted is by visiting kaggle.com.\\nAs mentioned throughout this book, Kaggle offers free datasets for\\ndownload. This saves you the time and effort of sourcing and formatting your\\nown dataset. Meanwhile, you also have the opportunity to discuss and\\nproblem-solve with other users on the forum, join competitions, and simply\\nhang out and talk about data.\\nBear in mind, however, that datasets you download from Kaggle will\\ninherently need some refining (through scrubbing) to tailor to the machine\\nlearning model that you decide to build. Below are four free sample datasets\\nfrom Kaggle that may prove useful to your further learning in this field.\\n \\nWorld Happiness Report\\nWhat countries rank the highest in overall happiness? Which factors\\ncontribute most to happiness? How did country rankings change between the\\n2015 and 2016 reports? Did any country experience a significant increase or\\ndecrease in happiness? These are the questions you can ask of this dataset\\nrecording happiness scores and rankings using data from the Gallup World\\nPoll. The scores are based on answers to the main life evaluation questions\\nasked in the poll.\\n \\nHotel Reviews\\nDoes having a five-star reputation lead to more disgruntled guests, and\\nconversely, can two-star hotels rock the guest ratings by setting low\\nexpectations and over-delivering? Or are one and two-star rated hotels simply\\nrated low for a reason? Find all this out from this sample dataset of hotel\\nreviews. This particular dataset covers 1,000 hotels and includes hotel name,\\nlocation, review date, text, title, username, and rating. The dataset is sourced\\nfrom the Datafiniti’s Business Database, which includes almost every hotel in', metadata={'source': 'ml.pdf', 'page': 124}),\n",
       " Document(page_content=\"the world.\\n \\nCraft Beers Dataset\\nDo you like craft beer? This dataset contains a list of 2,410 American craft\\nbeers and 510 breweries collected in January 2017 from CraftCans.com.\\nDrinking and data crunching is perfectly legal.\\n \\nBrazil's House of Deputies Reimbursements\\nAs politicians in Brazil are entitled to receive refunds from money spent on\\nactivities to “better serve the people,” there are interesting findings and\\nsuspicious outliers to be found in this dataset. Data on these expenses are\\npublicly available, but there is very little monitoring of expenses in Brazil. So\\ndon’t be surprised to see one public servant racking up over 800 flights in\\ntwelve months, and another that recorded R 140,000 (USD $44,500) on post\\nexpenses—yes, snail mail!\", metadata={'source': 'ml.pdf', 'page': 125}),\n",
       " Document(page_content='FINAL WORD\\nThank you for purchasing this book. You now have a baseline understanding\\nof the key concepts in machine learning and are ready to tackle this\\nchallenging subject in earnest. This includes learning the vital programming\\ncomponent of machine learning.\\nTo further your study of machine learning, I strongly recommend that you\\nenroll in the free Andrew Ng Machine Learning course offered on Coursera.\\nIf you have any direct feedback, both positive and negative, or suggestions to\\nimprove this book, please feel free to send me an email at\\noliver.theobald@scatterplotpress.com. This feedback is highly valued and I\\nlook forward to hearing from you.\\nFinally, I would like to express my gratitude to my colleagues Jeremy\\nPederson and Rui Xiong for their assistance in kindly sharing practical\\nmachine learning tips and some code used in this book.\\n \\nThank you,\\nOliver Theobald', metadata={'source': 'ml.pdf', 'page': 126}),\n",
       " Document(page_content='[1]\\n BBC, \\nWill A Robot Take My Job?\\n, 2015, http://www.bbc.com/news/technology-34066941\\n[2]\\n Nearshore Americas, Machine Learning Adoption Thwarted by Lack of Skills and Understanding, 2017, http://www.nearshoreamericas.com\\n[3]\\n Arthur Samuel,\\n \\nSome Studies in Machine Learning Using the Game of Checkers\\n, \\nIBM Journal of Research and Development, Vol. 3, Issue. 3, 1959.\\n[4]\\n Arthur Samuel,\\n \\nSome Studies in Machine Learning Using the Game of Checkers\\n, \\nIBM Journal of Research and Development, Vol. 3, Issue. 3, 1959.\\n[5]\\n DataVisor, Unsupervised Machine Learning Engine, 2017, https://www.datavisor.com/unsupervised-machine-learning-engine/\\n[6]\\n Kevin Kelly, The Inevitable: Understanding the 12 Technological Forces That Will Shape Our Future, Penguin Books, 2016.\\n[7]\\n Torch, What is Torch? http://torch.ch/, 2017', metadata={'source': 'ml.pdf', 'page': 127})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"ml.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Rima! Your name is... I'm afraid I don't know. I'm just an AI and I don't have access to personal information about individuals unless it's been explicitly provided. Can you please tell me your name again?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"name i am given was rima\",\n",
    "        \"question\": \"What is my name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a vector storage memory\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(pages, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='actions.\\nWhile this sounds simple enough, implementation is a much more difficult\\ntask and beyond the scope of an absolute beginner’s introduction to machine\\nlearning. Reinforcement learning algorithms aren’t covered in this book,\\nhowever, I will leave you with a link to a more comprehensive explanation of\\nreinforcement learning and Q-learning following the Pac-Man scenario.\\nhttps://inst.eecs.berkeley.edu/~cs188/sp12/projects/reinforcement/reinforcement.html', metadata={'source': 'ml.pdf', 'page': 20}),\n",
       " Document(page_content='# The final two columns of the table are not part of the original dataset and have been added for convenience to complete the following equation.\\nWhere:\\nΣ = Total sum\\nΣx = Total sum of all x values (1 + 2 + 1 + 4 + 3 = 11)\\nΣy = Total sum of all y values (3 + 4 + 2 + 7 + 5 = 21)\\nΣxy = Total sum of x*y for each row (3 + 8 + 2 + 28 + 15 = 56)\\nΣx\\n2 \\n= Total sum of x*x for each row (1 + 4 + 1 + 16 + 9 = 31)\\nn = Total number of rows. In the case of this example, n = 5.', metadata={'source': 'ml.pdf', 'page': 51}),\n",
       " Document(page_content='FINAL WORD\\nThank you for purchasing this book. You now have a baseline understanding\\nof the key concepts in machine learning and are ready to tackle this\\nchallenging subject in earnest. This includes learning the vital programming\\ncomponent of machine learning.\\nTo further your study of machine learning, I strongly recommend that you\\nenroll in the free Andrew Ng Machine Learning course offered on Coursera.\\nIf you have any direct feedback, both positive and negative, or suggestions to\\nimprove this book, please feel free to send me an email at\\noliver.theobald@scatterplotpress.com. This feedback is highly valued and I\\nlook forward to hearing from you.\\nFinally, I would like to express my gratitude to my colleagues Jeremy\\nPederson and Rui Xiong for their assistance in kindly sharing practical\\nmachine learning tips and some code used in this book.\\n \\nThank you,\\nOliver Theobald', metadata={'source': 'ml.pdf', 'page': 126}),\n",
       " Document(page_content='Format:\\n E-Book, Book\\nAuthor: \\nMark Lutz\\nSuggested Audience: \\nAll (with an interest in learning Python)\\nA comprehensive introduction to Python published by O’Reilly Media.\\n \\nHands-On Machine Learning with Scikit-Learn and TensorFlow:\\nConcepts, Tools, and Techniques to Build Intelligent Systems\\nFormat:\\n E-Book, Book\\nAuthor: \\nAurélien Géron\\nSuggested Audience: \\nAll (with an interest in programming in Python, Scikit-\\nLearn and TensorFlow)\\nAs a highly popular O’Reilly Media book written by machine learning\\nconsultant Aurélien Géron, this is an excellent advanced resource for anyone\\nwith a solid foundation of machine learning and computer programming.\\n \\n| Recommendation Systems |\\n \\nThe Netflix Prize and Production Machine Learning Systems: An Insider\\nLook\\nFormat:\\n Blog\\nAuthor: \\nMathworks\\nSuggested Audience: \\nAll\\nA very interesting blog article demonstrating how Netflix applies machine\\nlearning to form movie recommendations.\\n \\nRecommender Systems\\nFormat:\\n Coursera course\\nPresenter: \\nThe University of Minnesota\\nCost: \\nFree 7-day trial or included with $49 USD Coursera subscription\\nSuggested Audience: \\nAll\\nTaught by the University of Minnesota, this Coursera specialization covers\\nfundamental recommender system techniques including content-based and\\ncollaborative filtering as well as non-personalized and project-association\\nrecommender systems.\\n.', metadata={'source': 'ml.pdf', 'page': 121}),\n",
       " Document(page_content=\"Author: \\nThe BBC\\nSuggested Audience: \\nAll\\nCheck how safe your job is in the AI era leading up to the year 2035.\\n \\nSo You Wanna Be a Data Scientist? A Guide to 2015's Hottest Profession\\nFormat:\\n Blog\\nAuthor: \\nTodd Wasserman\\nSuggested Audience: \\nAll\\nExcellent insight into becoming a data scientist.\\n \\nThe Data Science Venn Diagram\\nFormat:\\n Blog\\nAuthor: \\nDrew Conway\\nSuggested Audience: \\nAll\\nThe popular 2010 data science diagram designed by Drew Conway.\", metadata={'source': 'ml.pdf', 'page': 123}),\n",
       " Document(page_content='BIAS & VARIANCE\\nAlgorithm selection is an important step in forming an accurate prediction\\nmodel, but deploying an algorithm with a high rate of accuracy can be a\\ndifficult balancing act. The fact that each algorithm can produce vastly\\ndifferent models based on the hyperparameters provided can lead to\\ndramatically different results. As mentioned earlier, hyperparameters are the\\nalgorithm’s settings, similar to the controls on the dashboard of an airplane or\\nthe knobs used to tune radio frequency—except hyperparameters are lines of\\ncode!\\n \\nFigure 1: Example of hyperparameters in Python for the algorithm gradient boosting\\n \\nA constant challenge in machine learning is navigating \\nunderfitting\\n and\\noverfitting\\n, which describe how closely your model follows the actual\\npatterns of the dataset. To understand underfitting and overfitting, you must\\nfirst understand \\nbias\\n and \\nvariance\\n.\\nBias refers to the gap between your predicted value and the actual value. In\\nthe case of high bias, your predictions are likely to be skewed in a certain\\ndirection away from the actual values. Variance describes how scattered your\\npredicted values are. Bias and variance can be best understood by analyzing\\nthe following visual representation.', metadata={'source': 'ml.pdf', 'page': 71}),\n",
       " Document(page_content=\"“New” in the top right-hand corner of the web application to create a new\\n“Notepad” project, and then select “Python 3.”\\nThe final step is to install the necessary libraries required to complete this\\nexercise. You will need to install Pandas and a number of libraries from\\nScikit-learn into the notepad.\\nIn machine learning, each project will vary in regards to the libraries required\\nfor import. For this particular exercise, we are using gradient boosting\\n(ensemble modeling) and mean absolute error to measure performance.\\nYou will need to import each of the following libraries and functions by\\nentering these exact commands in Jupyter Notebook:\\n \\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import ensemble\\nfrom sklearn.metrics import mean_absolute_error\\nfrom sklearn.externals import joblib\\n \\nDon’t worry if you don’t recognize each of the imported libraries in the code\\nsnippet above. These libraries will be referred to in later steps.\\n \\n2) Import the dataset\\nThe next step is to import the dataset. For this exercise, I have selected a free\\nand publicly available dataset from kaggle.com which contains house, unit,\\nand townhouse prices in Melbourne, Australia. This dataset comprises data\\nscraped from publicly available listings posted weekly on\\nwww.domain.com.au. The dataset contains 14,242 property listings and 21\\nvariables including address, suburb, land size, number of rooms, price,\\nlongitude, latitude, postcode, etc.\\nPlease note that the property values in this dataset are expressed in Australian\\nDollars—$1 AUD is approximately $0.77 USD (as of 2017).\\nDownload the Melbourne Housing Market dataset from this link:\\nhttps://www.kaggle.com/anthonypino/melbourne-housing-market\\nAfter registering a free account and logging into kaggle.com, download the\\ndataset as a zip file. Next, unzip the downloaded file and import into Jupyter\\nNotebook. To import the dataset, you can utilize the read_csv function to\\nload the data into a Pandas dataframe.\\n \\ndf = pd.read_csv('~/Downloads/Melbourne_housing_FULL-26-09-2017.csv')\", metadata={'source': 'ml.pdf', 'page': 102})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a data retriever from the vector store\n",
    "# Retriever is a component of Langchain that is used to retrieve the most relevant data from the data source\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "retriever.invoke(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided documents, deep learning refers to a subfield of machine learning that involves the use of artificial neural networks to model and solve complex problems. The term \"deep\" refers to the number of layers in the neural network, which can range from a few to hundreds or even thousands. Deep learning algorithms are capable of learning and representing complex patterns in data, such as images, speech, and text, and have been successful in a wide range of applications, including image recognition, natural language processing, and autonomous driving.\\n\\nThe documents provide some background information on machine learning and reinforcement learning, but do not go into detail on deep learning specifically. However, they do mention that reinforcement learning algorithms are not covered in the book, which suggests that deep reinforcement learning is a topic beyond the scope of the book.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    } | prompt | model | parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is deep learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning?\n",
      "Answer: Machine learning is a subfield of artificial intelligence (AI) that involves developing algorithms and statistical models that enable computers to learn from data, make decisions, and improve their performance on a specific task over time. In other words, machine learning is a technique that enables computers to learn and improve their performance on a particular task without being explicitly programmed for each task.\n",
      "\n",
      "Machine learning algorithms can be broadly classified into two types: supervised and unsupervised learning. Supervised learning involves training an algorithm on labeled data, where the output is known, and the algorithm learns to map inputs to outputs based on the patterns in the labeled data. Examples of supervised learning tasks include image classification, speech recognition, and sentiment analysis.\n",
      "\n",
      "Unsupervised learning involves training an algorithm on unlabeled data, where there are no known outputs. The algorithm learns to identify patterns or structure in the data without any prior knowledge of what the output should be. Examples of unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection.\n",
      "\n",
      "Some common machine learning algorithms include:\n",
      "\n",
      "1. Linear Regression: a statistical model that predicts a continuous outcome variable based on one or more predictor variables.\n",
      "2. Decision Trees: a tree-based model that splits the data into subsets based on the values of the input features and predicts the output value for each subset.\n",
      "3. Support Vector Machines (SVMs): a classifier that finds the hyperplane that maximally separates the data points of different classes.\n",
      "4. Neural Networks: a complex model composed of multiple layers of interconnected nodes (neurons) that learn to represent the data in a hierarchical manner.\n",
      "5. Naive Bayes: a probabilistic classifier that predicts the class of a new observation based on the conditional probability of the class given the input features.\n",
      "\n",
      "Machine learning has numerous applications in various fields, including:\n",
      "\n",
      "1. Healthcare: disease diagnosis, patient outcome prediction, and personalized medicine.\n",
      "2. Finance: fraud detection, credit risk assessment, and portfolio optimization.\n",
      "3. Marketing: customer segmentation, response prediction, and recommendation systems.\n",
      "4. Transportation: autonomous vehicles, route optimization, and traffic forecasting.\n",
      "5. Education: student performance prediction, personalized learning, and educational resource allocation.\n",
      "\n",
      "Overall, machine learning is a powerful tool for extracting insights and making predictions from large datasets, and it has the potential to revolutionize many industries and fields.\n",
      "\n",
      "Question: What is Supervised Learning?\n",
      "Answer: Supervised learning is a type of machine learning where the algorithm is trained using labeled data. The algorithm learns the relationship between the input features and the corresponding output labels, allowing it to make predictions on new, unseen data. In supervised learning, the training data consists of input-output pairs, where each input is associated with a particular output. The goal of supervised learning is to learn a mapping between the inputs and outputs that can be used to make accurate predictions on new data.\n",
      "\n",
      "Supervised learning is the most common type of machine learning and is widely used in applications such as image classification, speech recognition, and sentiment analysis. It is particularly useful when there is a well-defined target output or label for the algorithm to learn from.\n",
      "\n",
      "Some common types of supervised learning include:\n",
      "\n",
      "1. Linear Regression: A linear model that predicts a continuous output variable based on one or more input features.\n",
      "2. Logistic Regression: A non-linear model that predicts a binary output variable (e.g., 0 or 1) based on one or more input features.\n",
      "3. Decision Trees: A tree-based model that splits the data into smaller subsets based on the values of the input features and produces an output prediction for each subset.\n",
      "4. Random Forests: An ensemble model that combines multiple decision trees to produce a more accurate prediction.\n",
      "5. Support Vector Machines (SVMs): A non-linear model that finds the hyperplane that maximally separates the data points of different classes in the input space.\n",
      "\n",
      "In summary, supervised learning is a type of machine learning where the algorithm learns from labeled data to make predictions on new, unseen data. It is widely used in various applications and has many types of algorithms that can be used depending on the problem at hand.\n",
      "\n",
      "Question: What is Unsupervised Learning?\n",
      "Answer: The answer to your question can be found in the following document:\n",
      "\n",
      "Document(page_content='BIAS & VARIANCE\\nAlgorithm selection is an important step in forming an accurate prediction\\nmodel, but deploying an algorithm with a high rate of accuracy can be a\\ndifficult balancing act. The fact that each algorithm can produce vastly\\ndifferent models based on the hyperparameters provided can lead to\\ndramatically different results. As mentioned earlier, hyperparameters are the\\nalgorithm’s settings, similar to the controls on the dashboard of an airplane or\\nthe knobs used to tune radio frequency—except hyperparameters are lines of\\ncode!\\n \\nFigure 1: Example of hyperparameters in Python for the algorithm gradient boosting\\n \\nA constant challenge in machine learning is navigating \\nunderfitting\\n and\\noverfitting\\n, which describe how closely your model follows the actual\\npatterns of the dataset. To understand underfitting and overfitting, you must\\nfirst understand \\nbias\\n and \\nvariance\\n.\\nBias refers to the gap between your predicted value and the actual value. In\\nthe case of high bias, your predictions are likely to be skewed in a certain\\ndirection away from the actual values. Variance describes how scattered your\\npredicted values are. Bias and variance can be best understood by analyzing\\nthe following visual representation.', metadata={'source': 'ml.pdf', 'page': 71}).\n",
      "\n",
      "Unsupervised learning is a type of machine learning where the algorithm tries to find patterns or relationships in the data without any supervision or labeled examples. The goal is to identify structure in the data, such as clusters, dimensions, or anomalies, without prior knowledge of the expected output.\n",
      "\n",
      "Question: What is Reinforcement Learning?\n",
      "Answer: Reinforcement learning (RL) is a subfield of machine learning that focuses on training agents to make decisions in complex, uncertain environments. The goal of RL is to learn a policy that maps states to actions that maximize a cumulative reward signal. Unlike traditional supervised learning, where the goal is to learn a mapping between inputs and outputs, RL agents learn through trial and error by interacting with their environment.\n",
      "\n",
      "The key components of RL are:\n",
      "\n",
      "1. Agent: The entity that interacts with the environment.\n",
      "2. Environment: The external world that the agent interacts with.\n",
      "3. Actions: The actions taken by the agent in the environment.\n",
      "4. States: The current situation or status of the environment.\n",
      "5. Reward: A feedback signal that indicates the desirability of the agent's actions.\n",
      "6. Policy: The mapping from states to actions learned by the agent through experience.\n",
      "7. Value function: An estimate of the expected return of taking a particular action in a particular state.\n",
      "\n",
      "RL algorithms can be categorized into two types: model-based and model-free. Model-based RL algorithms learn a model of the environment and use this model to plan a sequence of actions to take. Model-free RL algorithms learn a policy directly from the observations and rewards without explicitly modeling the environment.\n",
      "\n",
      "Some popular RL algorithms include:\n",
      "\n",
      "1. Q-learning: A model-free algorithm that learns an estimate of the expected return for each action in each state.\n",
      "2. SARSA: A model-free algorithm that learns both the policy and the value function simultaneously.\n",
      "3. Deep Q-Networks (DQN): A model-free algorithm that uses a deep neural network to approximate the Q-function.\n",
      "4. Policy Gradient Methods: Model-free algorithms that learn the policy directly by optimizing the expected return.\n",
      "5. Actor-Critic Methods: Model-based algorithms that learn both the policy and the value function simultaneously.\n",
      "\n",
      "RL has many applications, including:\n",
      "\n",
      "1. Robotics: Training agents to control robots for tasks such as grasping and manipulation, navigation, and human-robot interaction.\n",
      "2. Game Playing: Using RL to train agents to play complex games like Go, poker, and video games.\n",
      "3. Recommendation Systems: Using RL to learn personalized recommendations for users based on their past behavior.\n",
      "4. Financial Trading: Training agents to make trading decisions based on market data.\n",
      "5. Autonomous Vehicles: Training agents to control self-driving cars and make decisions based on sensor data.\n",
      "\n",
      "In conclusion, reinforcement learning is a powerful tool for training agents to make decisions in complex, uncertain environments. By learning from trial and error, RL algorithms can optimize policies that maximize cumulative rewards and solve challenging problems in various fields.\n",
      "\n",
      "Question: What is Deep Learning?\n",
      "Answer:  Based on the provided documents, deep learning refers to a subset of machine learning that involves the use of artificial neural networks to model and solve complex problems. It is a more advanced and challenging area of machine learning that requires a solid foundation in programming and computer science, as well as a good understanding of key concepts such as reinforcement learning, Q-learning, and bias/variance.\n",
      "\n",
      "The documents provide various definitions and explanations of deep learning, including:\n",
      "\n",
      "* \"Deep learning is a subset of machine learning that involves the use of artificial neural networks to model and solve complex problems.\" (Document 1)\n",
      "* \"Deep learning is a branch of machine learning that combines computational power and sophisticated algorithms to analyze and interpret large amounts of data.\" (Document 2)\n",
      "* \"Deep learning is a subfield of machine learning that focuses on developing artificial neural networks with multiple layers to model and solve complex problems.\" (Document 3)\n",
      "\n",
      "Overall, deep learning is a powerful tool for analyzing and making predictions about complex data sets, and it has many applications in fields such as computer vision, natural language processing, and speech recognition.\n",
      "\n",
      "Question: What is Neural Networks?\n",
      "Answer:  Based on the provided context, I can answer your question. Neural Networks are covered in the document \"Hands-On Machine Learning with Scikit-Learn and TensorFlow\" by Aurélien Géron. The document suggests that Neural Networks are an advanced machine learning technique that involves training a computer model to recognize patterns in data, just like the human brain. The document also explains that Neural Networks are composed of multiple layers of interconnected nodes (also called neurons), which process and transmit information.\n",
      "\n",
      "Question: What is Convolutional Neural Networks?\n",
      "Answer: Based on the provided documents, Convolutional Neural Networks (CNN) are a type of neural network architecture that are particularly well-suited for image and video analysis tasks. They are designed to take advantage of the spatial structure in images by applying a set of filters that scan the image in a sliding window fashion. This allows the network to extract features that are useful for image classification, object detection, and other computer vision tasks.\n",
      "\n",
      "The documents provide some information on CNNs, including their definition, applications, and how they work. Here are some key points:\n",
      "\n",
      "1. Definition: CNNs are a type of neural network architecture that are designed to process data with grid-like topology, such as images. They use convolutional layers to extract features from the input data.\n",
      "2. Applications: CNNs have been successfully applied to various computer vision tasks such as image classification, object detection, segmentation, and generation.\n",
      "3. How they work: CNNs consist of multiple convolutional layers that slide over the input data, applying filters that detect increasingly complex features. Each layer learns to detect a different set of features, allowing the network to capture a hierarchical representation of the input data. The output of each layer is passed through a nonlinear activation function to introduce nonlinearity in the model.\n",
      "4. Importance: CNNs have revolutionized the field of computer vision and have led to significant advances in various applications such as facial recognition, self-driving cars, medical imaging, and more.\n",
      "\n",
      "In summary, CNNs are a powerful tool for image and video analysis tasks, and have been successfully applied to various applications in computer vision.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is Supervised Learning?\",\n",
    "    \"What is Unsupervised Learning?\",\n",
    "    \"What is Reinforcement Learning?\",\n",
    "    \"What is Deep Learning?\",\n",
    "    \"What is Neural Networks?\",\n",
    "    \"What is Convolutional Neural Networks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, bias and weights seem to be related to machine learning concepts. Here is what I know about them based on the given documents:\n",
      "\n",
      "* Bias: In machine learning, bias refers to the gap between predicted values and actual values. It measures the error caused by an algorithm's assumptions or simplifications in representing a complex problem. Understanding bias is crucial for navigating underfitting and overfitting (Document(page_content='BIAS & VARIANCE', metadata={'source': 'ml.pdf', 'page': 51})).\n",
      "* Weights: In machine learning, weights refer to the algorithm's settings or lines of code that control various aspects of a model's behavior (Document(page_content='metrics\\nAlgorithm selection is an important step in forming an accurate prediction\\nmodel, but deploying an algorithm with a high rate of accuracy can be a\\ndifficult balancing act. The fact that each algorithm can produce vastly\\ndifferent models based on the hyperparameters provided can lead to\\ndramatically different results' metadata={'source': 'ml.pdf', 'page': 121}).\n",
      "\n",
      "Based on these documents, it seems that bias and weights are related concepts in machine learning. Bias refers to the error caused by an algorithm's assumptions or simplifications, while weights refer to the algorithm's settings or lines of code that control various aspects of a model's behavior."
     ]
    }
   ],
   "source": [
    "# Best way works like answer generation of gpt\n",
    "for s in chain.stream({\"question\": \"What are bias and weights?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
